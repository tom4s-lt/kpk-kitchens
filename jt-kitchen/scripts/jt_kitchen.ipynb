{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLo1BmaH03uY"
      },
      "source": [
        "# Intro & Instructions\n",
        "\n",
        "Jupyter notebook for data extraction and processing for Joint Treasury data analysis. **Execution is on Colab** (not locally).\n",
        "\n",
        "Setup is done in the first section in order to have proper config for the whole nobtebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35zmnTpcw1Lu"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Setup all the required variables & logic for the notebook.\n",
        "\"\"\"\n",
        "%%capture  # To clear the output from the cell\n",
        "\n",
        "# ==============================================\n",
        "#  Install Required Packages\n",
        "# ==============================================\n",
        "\n",
        "! pip install dune_spice  # Install dune_spice for querying data from Dune Analytics\n",
        "# !pip install dune_client\n",
        "# !pip install dataclasses_json\n",
        "\n",
        "# ==============================================\n",
        "#  Import Required Libraries\n",
        "# ==============================================\n",
        "\n",
        "# Google authentication libraries\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from google.auth import default\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Other libraries\n",
        "from dune_client.types import QueryParameter\n",
        "from dune_client.client import DuneClient\n",
        "from dune_client.query import QueryBase\n",
        "import spice\n",
        "\n",
        "import pandas as pd\n",
        "import polars\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "import pprint\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import time\n",
        "import os\n",
        "\n",
        "# ==============================================\n",
        "#  Configuration Class\n",
        "# ==============================================\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Configuration class containing all constants and settings for the script.\"\"\"\n",
        "    \n",
        "    # Environment Variables and API Keys\n",
        "    DUNE_API_KEY: str = 'RyXNIYLH4uE5NeEjLWQBZEcrkjTRw2EH'\n",
        "    COINGECKO_API_KEY: str = 'CG-jN5KXD1QFHacbpJb3T7PVJ3P'\n",
        "    \n",
        "    # API Endpoints\n",
        "    COINGECKO_BASE_URL: str = \"https://api.coingecko.com/api/v3\"\n",
        "    COINGECKO_PRICE_ENDPOINT: str = f\"{COINGECKO_BASE_URL}/simple/price?vs_currencies=usd&ids=\"\n",
        "    \n",
        "    # File Paths and Directories\n",
        "    DATA_DIR: str = \"./data\"\n",
        "    \n",
        "    # Google Sheets Configuration\n",
        "    WORKBOOK_URL: str = \"https://docs.google.com/spreadsheets/d/1mIQTla9L7l3FBh1k8xtpHQwHMN4D7nHl1u5nsdeWdA0/\"\n",
        "    \n",
        "    # Important Dates\n",
        "    TTE_DATE: str = \"2024-04-23\"\n",
        "    ETL_NOW: datetime = pd.to_datetime(datetime.now())\n",
        "    \n",
        "    # API Request Settings\n",
        "    MAX_RETRIES: int = 3\n",
        "    DEFAULT_TIMEOUT: int = 10\n",
        "    RETRY_DELAY: int = 5\n",
        "    \n",
        "    # Token Configurations\n",
        "    PORTFOLIO_TOKENS = {\n",
        "        'coingecko_id': {\n",
        "            # Stablecoins\n",
        "            'dai': 'dai',\n",
        "            'xdai': 'xdai',\n",
        "            'usdc': 'usd-coin',\n",
        "            'usdt': 'tether',\n",
        "            # ETH\n",
        "            'eth': 'ethereum',\n",
        "            # SAFE & GNO\n",
        "            'safe': 'safe',\n",
        "            'gno': 'gnosis',\n",
        "            # Other Tokens\n",
        "            'aura': 'aura-finance',\n",
        "            'bal': 'balancer'\n",
        "        },\n",
        "        'asset_class': {\n",
        "            'dai': 'stablecoins',\n",
        "            'xdai': 'stablecoins',\n",
        "            'usdc': 'stablecoins',\n",
        "            'usdt': 'stablecoins',\n",
        "            'eth': 'eth',\n",
        "            'weth': 'eth',\n",
        "            'safe': 'safe',\n",
        "            'gno': 'gno',\n",
        "            'aura': 'other',\n",
        "            'bal': 'other'\n",
        "        },\n",
        "        'similar_tokens': {  # Coingecko ID for similar tokens (to compare market data)\n",
        "            'safe': 'safe',\n",
        "            'gno': 'gnosis',\n",
        "            'eth': 'ethereum',\n",
        "            'uni': 'uniswap',\n",
        "            'link': 'chainlink',\n",
        "            'grt': 'the-graph'\n",
        "        },\n",
        "        'token_class': {  # mapper from token to asset for matching with current price\n",
        "            'SAFE': 'safe',\n",
        "            'GNO': 'gno',\n",
        "            'WETH': 'eth',\n",
        "            'USDC': 'usdc'\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Token Address Mappings\n",
        "    TOKEN_ADDRESSES = {\n",
        "        '0X5AFE3855358E112B5647B952709E6165E1C1EEEE': 'SAFE',  # Ethereum\n",
        "        '0X4D18815D14FE5C3304E87B3FA18318BAA5C23820': 'SAFE',  # Gnosis\n",
        "        '0XC02AAA39B223FE8D0A0E5C4F27EAD9083C756CC2': 'ETH',  # Ethereum WETH\n",
        "        '0X9C58BACC331C9AA871AFD802DB6379A98E80CEDB': 'GNO',  # Gnosis\n",
        "        '0XA0B86991C6218B36C1D19D4A2E9EB0CE3606EB48': 'USDC',  # Ethereum\n",
        "        '0X5AFE3855358E112B5647B952709E6165E1C1EEEE-0XC02AAA39B223FE8D0A0E5C4F27EAD9083C756CC2': 'SAFE-ETH',  # Ethereum\n",
        "        '0X5AFE3855358E112B5647B952709E6165E1C1EEEE-0XA0B86991C6218B36C1D19D4A2E9EB0CE3606EB48': 'SAFE-USDC',  # Ethereum\n",
        "        '0X4D18815D14FE5C3304E87B3FA18318BAA5C23820-0X9C58BACC331C9AA871AFD802DB6379A98E80CEDB': 'SAFE-GNO'  # Gnosis\n",
        "    }\n",
        "    \n",
        "    # Plot Settings\n",
        "    @staticmethod\n",
        "    def setup_plot_style():\n",
        "        plt.style.use('seaborn-v0_8')\n",
        "        sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "# ==============================================\n",
        "#  Create directory for local/colab execution\n",
        "# ==============================================\n",
        "\n",
        "# Create the data directory\n",
        "os.makedirs(Config.DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Setup plot style\n",
        "Config.setup_plot_style()\n",
        "\n",
        "# Initialize pretty printer\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "# Open workbook\n",
        "wb = gc.open_by_url(Config.WORKBOOK_URL)\n",
        "\n",
        "# ==============================================================================\n",
        "# Function definitions\n",
        "# ==============================================================================\n",
        "\n",
        "# Directly generate df from gsheet imports\n",
        "\n",
        "def etl_gen_df_from_gsheet(wb_url, page, output_type = 'json', index_col = ''):\n",
        "  \"\"\"Generates dataframe from a table stored in google sheets\n",
        "  str dtype is forced for conversion to decimal.Decimal type with all decimals\n",
        "\n",
        "  Args:\n",
        "    wb_url (str): url of the spreadsheet\n",
        "    page (str): name of the page that has to be accessed\n",
        "    output_type (str): type of output desired (json/df)\n",
        "    index_col (str): col_label to be used as string\n",
        "\n",
        "  Returns:\n",
        "    df (DataFrame): containing information on the page\n",
        "  \"\"\"\n",
        "\n",
        "  wb = gc.open_by_url(wb_url)\n",
        "\n",
        "  sheet = wb.worksheet(page)\n",
        "  records = sheet.get_all_records()\n",
        "\n",
        "  if output_type == 'df':\n",
        "    df = pd.DataFrame(records, dtype = str)\n",
        "    if index_col == '':\n",
        "      pass\n",
        "    else:\n",
        "      df.set_index(index_col, drop = True, inplace = True)\n",
        "\n",
        "    output = df\n",
        "\n",
        "  else:\n",
        "\n",
        "    output = records\n",
        "\n",
        "  return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3zzLKCxlRS3"
      },
      "source": [
        "# DATA - Ext"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Bmwgx1dPV9"
      },
      "source": [
        "Extracting and handling data to be exported to the JT Sheet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyZ-vy7gWKpi"
      },
      "source": [
        "## CG - prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzBtUTajWKQQ",
        "outputId": "c8cace5d-2095-4f39-fcd9-3cc799fa3ff4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-1fd5175eb9f5>:68: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  sheet.update(\"A1\", prices_df_export_list)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Seeks current prices for portfolio tokens or other tokens of interest - CoinGecko API\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction\n",
        "# ==============================================================================\n",
        "\n",
        "# Build the string that contains all token IDs to make the API call\n",
        "id_string = ''\n",
        "for id in Config.PORTFOLIO_TOKENS['coingecko_id']:\n",
        "    id_string = id_string + Config.PORTFOLIO_TOKENS['coingecko_id'][id] + '%2C'\n",
        "\n",
        "id_string = id_string[:-3]  # Exclude the last '%2C' from the call - it makes it crash\n",
        "\n",
        "params = {\n",
        "    'x_cg_demo_api_key': Config.COINGECKO_API_KEY\n",
        "}\n",
        "\n",
        "r = requests.get(Config.COINGECKO_PRICE_ENDPOINT + id_string, params=params)\n",
        "\n",
        "# ==============================================================================\n",
        "# Request handling\n",
        "# ==============================================================================\n",
        "\n",
        "df = pd.DataFrame(r.json())\n",
        "\n",
        "# Mapping from coingecko ids to asset\n",
        "inv_map = {v: k for k, v in Config.PORTFOLIO_TOKENS['coingecko_id'].items()}\n",
        "\n",
        "# Rename the columns from coingecko_id to symbol\n",
        "df = df.rename(columns=inv_map).T\n",
        "df.columns = ['price']\n",
        "\n",
        "# Add the asset class for each asset - this is used later in the JT sheet for asset wide calculations\n",
        "df['asset_class'] = df.index.map(Config.PORTFOLIO_TOKENS['asset_class'])\n",
        "\n",
        "# Add specific cases listed in the dictionary in the setup index\n",
        "df.loc['weth', :] = df.loc['eth', :]\n",
        "\n",
        "# Add datetime + etl_dt (etl datetime for reference of when info was fetched)\n",
        "df['datetime'] = Config.ETL_NOW\n",
        "df['datetime'] = df['datetime'].dt.floor('s')\n",
        "df['etl_dt'] = df['datetime']\n",
        "\n",
        "# ==============================================================================\n",
        "# Data export\n",
        "# ==============================================================================\n",
        "\n",
        "prices_df = df.copy()\n",
        "\n",
        "# Open sheet inside workbook\n",
        "sheet = wb.worksheet('prices')\n",
        "\n",
        "# Convert relevant columns in dataframe to str to export\n",
        "prices_df = prices_df.astype({'datetime': str, 'etl_dt': str})\n",
        "\n",
        "# Reset index for all data to appear - symbols were the index\n",
        "prices_df = prices_df.reset_index()\n",
        "\n",
        "# DataFrame export\n",
        "prices_df_export_list = [prices_df.columns.tolist()]  # Column labels\n",
        "prices_df_export_list.extend(prices_df.values.tolist())\n",
        "sheet.update(\"A1\", prices_df_export_list)\n",
        "\n",
        "# Export to csv\n",
        "prices_df.to_csv(f\"{Config.DATA_DIR}/prices.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdox00HD3FTY"
      },
      "source": [
        "## CG - tickers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "9bfPevAb3Kqh",
        "outputId": "79a10328-b36b-4eae-cc76-14c4745ff1aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-9d03868f5851>:120: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  sheet.update(\"A1\", tickers_df_export_list)\n"
          ]
        },
        {
          "ename": "InvalidJSONError",
          "evalue": "Out of range float values are not JSON compliant",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_body\u001b[0;34m(self, data, files, json)\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 510\u001b[0;31m                 \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    511\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, encoding, default, use_decimal, namedtuple_as_object, tuple_as_array, bigint_as_string, sort_keys, item_sort_key, for_json, ignore_nan, int_as_string_bitcount, iterable_as_array, **kw)\u001b[0m\n\u001b[1;32m    377\u001b[0m     ):\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;31m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/simplejson/encoder.py\u001b[0m in \u001b[0;36miterencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Out of range float values are not JSON compliant",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mInvalidJSONError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9d03868f5851>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0mtickers_df_export_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtickers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Column labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0mtickers_df_export_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtickers_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0msheet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtickers_df_export_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Export to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gspread/worksheet.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, values, range_name, raw, major_dimension, value_input_option, include_values_in_response, response_value_render_option, response_date_time_render_option)\u001b[0m\n\u001b[1;32m   1244\u001b[0m         }\n\u001b[1;32m   1245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1246\u001b[0;31m         response = self.client.values_update(\n\u001b[0m\u001b[1;32m   1247\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspreadsheet_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0mfull_range_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gspread/http_client.py\u001b[0m in \u001b[0;36mvalues_update\u001b[0;34m(self, id, range, params, body)\u001b[0m\n\u001b[1;32m    171\u001b[0m         \"\"\"\n\u001b[1;32m    172\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPREADSHEET_VALUES_URL\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"put\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/gspread/http_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, endpoint, params, data, json, files, headers)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMutableMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     ) -> Response:\n\u001b[0;32m--> 114\u001b[0;31m         response = self.session.request(\n\u001b[0m\u001b[1;32m    115\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/auth/transport/requests.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, headers, max_allowed_time, timeout, **kwargs)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTimeoutGuard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining_time\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mguard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m             response = super(AuthorizedSession, self).request(\n\u001b[0m\u001b[1;32m    538\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         )\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mprep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m         p.prepare(\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare\u001b[0;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mprepare_body\u001b[0;34m(self, data, files, json)\u001b[0m\n\u001b[1;32m    510\u001b[0m                 \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidJSONError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidJSONError\u001b[0m: Out of range float values are not JSON compliant"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Seeks current market data form CoinGecko - the table below each asset on their respective page\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction\n",
        "# ==============================================================================\n",
        "\n",
        "asset = 'safe'\n",
        "\n",
        "endpoint = f\"{Config.COINGECKO_BASE_URL}/coins/{asset}/tickers?depth=true\"\n",
        "\n",
        "params = {\n",
        "    'x_cg_demo_api_key': Config.COINGECKO_API_KEY\n",
        "}\n",
        "\n",
        "r = requests.get(endpoint, params=params)\n",
        "\n",
        "# ==============================================================================\n",
        "# Request handling\n",
        "# ==============================================================================\n",
        "\n",
        "tickers = r.json()['tickers']\n",
        "\n",
        "data = []\n",
        "\n",
        "for ticker in range(len(tickers)):\n",
        "    data_temp = {\n",
        "        'exchange': tickers[ticker]['market']['name'],\n",
        "        'pair': tickers[ticker]['base'] + '-' + tickers[ticker]['target'],\n",
        "        'price': tickers[ticker]['last'],\n",
        "        'spread': tickers[ticker]['bid_ask_spread_percentage'],  # it's in percentage, have to pass to decimal\n",
        "        '2_pct': tickers[ticker]['cost_to_move_up_usd'],\n",
        "        '-2_pct': tickers[ticker]['cost_to_move_down_usd'],\n",
        "        '24h_vol': tickers[ticker]['converted_volume']['usd'],\n",
        "        'trust_score': tickers[ticker]['trust_score']\n",
        "    }\n",
        "    data.append(data_temp)\n",
        "\n",
        "df = pd.DataFrame.from_records(data)\n",
        "\n",
        "# Extract blockchain from DEXs by extracting it from the Exchange name\n",
        "df['chain'] = df['exchange'].str.extract(r'\\((.*?)\\)', expand=False)\n",
        "\n",
        "# Add tye of exchange (CEX/DEX)\n",
        "df['type'] = df['chain'].apply(lambda x: 'DEX' if pd.notnull(x) else 'CEX')\n",
        "\n",
        "# Add Volume percentage for each Pair\n",
        "df['vol_pct'] = df['24h_vol'].div(df['24h_vol'].sum())\n",
        "\n",
        "# Add datetime + etl_dt (etl datetime for reference of when info was fetched)\n",
        "df['datetime'] = Config.ETL_NOW\n",
        "df['datetime'] = df['datetime'].dt.floor('s')\n",
        "df['etl_dt'] = df['datetime']\n",
        "\n",
        "# Add vol*-2_pct - for volume weighted -2% size\n",
        "df['depth_*_vol'] = df['-2_pct'] * df['24h_vol']\n",
        "\n",
        "# Add Notes column to match the JT Sheet table that has notes\n",
        "df['notes'] = \"\"\n",
        "\n",
        "# Replace anomally signs in the 'pair' column\n",
        "df['pair'] = df['pair'].str.replace('$', '')\n",
        "df['pair'] = df['pair'].str.replace('SAFE1', 'SAFE')\n",
        "\n",
        "# Replace token addresses\n",
        "df['pair'] = df['pair'].replace(regex=Config.TOKEN_ADDRESSES)\n",
        "\n",
        "# Replace NaN for \"nan\" for correct data for exporting to the Sheet - the same string that is fetched from the import\n",
        "df['chain'] = df['chain'].fillna('nan')\n",
        "\n",
        "# Order as the sheets table - it's to append below\n",
        "df = df[['exchange', 'type', 'pair', 'price', 'spread', '2_pct', '-2_pct', '24h_vol', 'vol_pct', 'trust_score', 'chain', 'depth_*_vol', 'datetime', 'etl_dt', 'notes']]\n",
        "\n",
        "# ==============================================================================\n",
        "# Data export\n",
        "# ==============================================================================\n",
        "\n",
        "tickers_df = df.copy()\n",
        "\n",
        "# Open sheet inside workbook\n",
        "sheet = wb.worksheet('tickers')\n",
        "\n",
        "# Convert dateitme in dataframe to str to export\n",
        "tickers_df = tickers_df.astype({'datetime': str, 'etl_dt': str})\n",
        "\n",
        "# Reset index for all data to appear\n",
        "tickers_df = tickers_df.reset_index()\n",
        "\n",
        "# Get current data from the tab (it's a timeseries that gets updated with latest data) - plus convert imported datatypes (only the relevant ones)\n",
        "past_tickers_df = etl_gen_df_from_gsheet(wb_url=Config.WORKBOOK_URL, page='tickers', output_type='df')\n",
        "\n",
        "# Convert data from all rows that should be numeric to numeric - convert to np.nan if impossible\n",
        "past_tickers_df['spread'] = pd.to_numeric(past_tickers_df['spread'], errors='coerce')\n",
        "past_tickers_df['2_pct'] = pd.to_numeric(past_tickers_df['2_pct'], errors='coerce')\n",
        "past_tickers_df['vol_pct'] = pd.to_numeric(past_tickers_df['vol_pct'], errors='coerce')\n",
        "\n",
        "# Convert all columns to their respective dtype for appending the new information\n",
        "past_tickers_df = past_tickers_df.astype({\n",
        "    'price': float,\n",
        "    'spread': float,\n",
        "    '2_pct': float,\n",
        "    '2_pct': float,\n",
        "    '24h_vol': float,\n",
        "    'vol_pct': float,\n",
        "    'depth_*_vol': float\n",
        "})\n",
        "\n",
        "# Join the historical and new dataframes\n",
        "tickers_df = pd.concat([past_tickers_df, tickers_df], axis=0)  # Generate records dataframe to export\n",
        "\n",
        "# Convert relevant columns in dataframe to str to export\n",
        "tickers_df = tickers_df.astype({'spread': str, '2_pct': str, 'vol_pct': str, 'trust_score': str, 'chain': str, 'datetime': str, 'etl_dt': str})\n",
        "\n",
        "# DataFrame export\n",
        "tickers_df_export_list = [tickers_df.columns.tolist()]  # Column labels\n",
        "tickers_df_export_list.extend(tickers_df.values.tolist())\n",
        "sheet.update(\"A1\", tickers_df_export_list)\n",
        "\n",
        "# Export to csv\n",
        "tickers_df.to_csv(f\"{Config.DATA_DIR}/tickers.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtV2goMkeaMD"
      },
      "source": [
        "## CG & Dune - market_data\n",
        "\n",
        "- this can be optimized with a dune extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "b32FWoGtcU3n"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Seeks SAFE + similar tokens market data in CoinGecko (since the TTE)\n",
        "Also SAFE DEX Volume to understand the share of trade in each\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "# CoinGecko Call\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction & handling\n",
        "# ==============================================================================\n",
        "\n",
        "# Total number of days to look for market data (from tte to today)\n",
        "days = datetime.today() - datetime.strptime(Config.TTE_DATE, \"%Y-%m-%d\")\n",
        "\n",
        "data = []\n",
        "\n",
        "for id in Config.PORTFOLIO_TOKENS['similar_tokens']:\n",
        "    endpoint = f\"{Config.COINGECKO_BASE_URL}/coins/{Config.PORTFOLIO_TOKENS['similar_tokens'][id]}/market_chart?\"\n",
        "    params = {\n",
        "        'vs_currency': 'usd',\n",
        "        'days': days.days + 2,  # Then for the shift() method to leave info on first day and have a buffer\n",
        "        'interval': 'daily',\n",
        "        'x_cg_demo_api_key': Config.COINGECKO_API_KEY\n",
        "    }\n",
        "\n",
        "    r = requests.get(endpoint, params=params)\n",
        "\n",
        "    # Process price data\n",
        "    price = pd.DataFrame(r.json()['prices'], columns=['date', 'price'])  # Opening price for the day (first day is not standard)\n",
        "    price['date'] = pd.to_datetime(price['date'], unit='ms')\n",
        "    price = price.set_index('date')\n",
        "    price = price.resample('D', kind='period').last()\n",
        "\n",
        "    # Process market cap data\n",
        "    mcap = pd.DataFrame(r.json()['market_caps'], columns=['date', 'mcap'])  # Mcap for that price\n",
        "    mcap['date'] = pd.to_datetime(mcap['date'], unit='ms')\n",
        "    mcap = mcap.set_index('date')\n",
        "    mcap = mcap.resample('D', kind = 'period').last()\n",
        "\n",
        "    # Process volume data\n",
        "    volume = pd.DataFrame(r.json()['total_volumes'], columns = ['date', 'volume'])  # Volume of the last day\n",
        "    volume['date'] = pd.to_datetime(volume['date'], unit = 'ms')\n",
        "    volume = volume.set_index('date')\n",
        "    volume = volume.resample('D', kind = 'period').last()\n",
        "\n",
        "    # Combine all data & generate asset attribute\n",
        "    df = pd.concat([price, mcap, volume], axis = 'columns')\n",
        "    df['asset'] = id\n",
        "\n",
        "    # Adjust data to reflect the data consistently -  values for each day are actualy the finishing for the earlier one\n",
        "    df.columns = ['open', 'mcap', 'volume', 'asset']\n",
        "    df['volume'] = df['volume'].shift(-1)\n",
        "    df['close'] = df['open'].shift(-1)\n",
        "    df['mcap'] = df['mcap'].shift(-1)\n",
        "\n",
        "    # Organize & slice for dates since TTE\n",
        "    df = df[['asset', 'open', 'close', 'mcap', 'volume']]\n",
        "\n",
        "    # Missing today's close so row is removed\n",
        "    df = df.loc[tte_date:].iloc[:-1]\n",
        "\n",
        "    data.append(df)\n",
        "    print('Asset Price Fetched:', df['asset'].iloc[0].upper())\n",
        "    time.sleep(3)\n",
        "\n",
        "cg_df = pd.concat(data, axis = 0)\n",
        "\n",
        "# Additional Calcs\n",
        "cg_df['vol/mcap'] = cg_df['volume'] / cg_df['mcap']\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "# Dune API Call - SAFE DEX Data\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction & handling\n",
        "# ==============================================================================\n",
        "\n",
        "query_id = 3777854  # SAFE - * - GMD - $SAFE DEX Trading Volume (Pool)\n",
        "\n",
        "df = spice.query(query_id, refresh=True)\n",
        "query_result = df.to_pandas()\n",
        "\n",
        "# Make date index to resample and SUM data\n",
        "query_result['date'] = pd.to_datetime(query_result['block_date'])\n",
        "\n",
        "# Daily aggregation of numeric data\n",
        "dex_df = query_result.resample('D', kind = 'period', on = 'date').sum(numeric_only = True)\n",
        "\n",
        "# Select & Rename columns\n",
        "dex_df = dex_df[['trades', 'total_vol']]\n",
        "dex_df.columns = ['dex_trades', 'dex_vol']\n",
        "\n",
        "# ==============================================================================\n",
        "# Joining CG + DEX Data & final things\n",
        "# ==============================================================================\n",
        "\n",
        "# Add asset col for joining\n",
        "dex_df['asset'] = 'safe'\n",
        "\n",
        "# DF Merge - the total_df DataFrame will also be used later\n",
        "total_df = cg_df.merge(dex_df, how = 'left', on = ['date', 'asset'])\n",
        "\n",
        "# CEX Volume Calculations\n",
        "total_df['cex_vol'] = total_df['volume'] - total_df['dex_vol']\n",
        "\n",
        "# Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "total_df['etl_dt'] = etl_now\n",
        "total_df['etl_dt'] = total_df['etl_dt'].dt.floor('s')\n",
        "\n",
        "# ==============================================================================\n",
        "# Data export\n",
        "# ==============================================================================\n",
        "\n",
        "market_data_df = total_df.copy()\n",
        "\n",
        "# Open sheet inside workbook\n",
        "sheet = wb.worksheet('market_data')\n",
        "\n",
        "# Reset index for all data to appear - symbols were the index\n",
        "market_data_df = market_data_df.reset_index()\n",
        "\n",
        "# Convert relevant columns in dataframe to str to export - plus additional NaN values to nan in string\n",
        "market_data_df = market_data_df.astype({'date': str, 'etl_dt': str})\n",
        "market_data_df = market_data_df.fillna('nan')\n",
        "\n",
        "# DataFrame export\n",
        "market_data_df_export_list = [market_data_df.columns.tolist()]  # Column labels\n",
        "market_data_df_export_list.extend(market_data_df.values.tolist())\n",
        "\n",
        "sheet.update(\"A1\", market_data_df_export_list)\n",
        "\n",
        "# Export to csv\n",
        "market_data_df.to_csv('market_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_72pOAd4BT8Q"
      },
      "source": [
        "## Dune - dex_fees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWXFFJs2BXu8"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# Execute and get results of the Dune Query for DEX Fees\n",
        "# \"\"\"\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Request construction\n",
        "# # ==============================================================================\n",
        "\n",
        "# query_id = 3768733  # SAFE - * - GMD - $SAFE DEX Trading Fees\n",
        "\n",
        "# df = spice.query(query_id, refresh=True)\n",
        "# query_result = df.to_pandas()\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Request handling\n",
        "# # ==============================================================================\n",
        "\n",
        "# # Multiplty by current price to get current value of fees (for comparison with Revert)\n",
        "# query_result['fee_amount_usd_adj'] = query_result['token'].replace(regex = portfolio_tokens_v2['token_class'])  # replace token by the asset for price\n",
        "\n",
        "# \"\"\"Temporal Solution for Metis appearance\n",
        "# \"\"\"\n",
        "# query_result = query_result.loc[query_result['fee_amount_usd_adj'] not in ['Metis', 'OLAS']]\n",
        "\n",
        "# prices_ = prices_df.set_index('index')['price'].to_dict()  # get prices from the DataFrame\n",
        "\n",
        "# query_result['fee_amount_usd_adj'] = query_result['fee_amount_usd_adj'].replace(regex = prices_).astype(float)  # replace with prices\n",
        "# query_result['fee_amount_token'] = query_result['fee_amount_token'].replace('<nil>', 0)  # replace Dune nulls with 0\n",
        "# query_result['fee_amount_usd_adj'] = query_result['fee_amount_usd_adj'] * query_result['fee_amount_token'].astype(float)  # multiply amount by price\n",
        "\n",
        "# # Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "# query_result['etl_dt'] = etl_now\n",
        "# query_result['etl_dt'] = query_result['etl_dt'].dt.floor('s')\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Data export\n",
        "# # ==============================================================================\n",
        "\n",
        "# dex_fees = query_result.copy()\n",
        "\n",
        "# # Open sheet inside workbook\n",
        "# sheet = wb.worksheet('dex_fees')\n",
        "\n",
        "# # Convert relevant columns in dataframe to str to export - plus additional NaN values to nan in string\n",
        "# dex_fees = dex_fees.astype({'block_date': str, 'etl_dt': str})\n",
        "# dex_fees = dex_fees.fillna('nan')\n",
        "\n",
        "# # Reset index for all data to appear - symbols were the index\n",
        "# dex_fees = dex_fees.reset_index()\n",
        "\n",
        "# # DataFrame export\n",
        "# dex_fees_export_list = [dex_fees.columns.tolist()]  # Column labels\n",
        "# dex_fees_export_list.extend(dex_fees.values.tolist())\n",
        "# sheet.update(\"A1\", dex_fees_export_list)\n",
        "\n",
        "# # Export to csv\n",
        "# dex_fees.to_csv('dex_fees.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUpbs9252E7b"
      },
      "source": [
        "## Dune - dex_trades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G68ouinTfRFm"
      },
      "source": [
        "- Requires executing the Market Data script before\n",
        "- Needed from query\n",
        "  - block_date\n",
        "  - blockchain\n",
        "  - label\n",
        "  - token_sold_symbol\n",
        "  - token_bought_amount\n",
        "  - token_sold_amount\n",
        "  - amount_usd\n",
        "  - tx_from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3Xe_YCcl2EuI"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Seeks trading activity from Dune analytics - to gather data on it\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction\n",
        "# ==============================================================================\n",
        "\n",
        "query_id = 3768636  # SAFE - * - GMD - $SAFE DEX Trades (ref)\n",
        "\n",
        "df = spice.query(query_id, refresh=True)\n",
        "query_result = df.to_pandas()  # it's received in a polars dataframe\n",
        "\n",
        "# get most recent query results using raw sql\n",
        "# df = spice.query('SELECT * FROM ethereum.blocks LIMIT 5')\n",
        "\n",
        "# ==============================================================================\n",
        "# Request handling\n",
        "# ==============================================================================\n",
        "\n",
        "# The dex_trades DataFrame will also be used in the other metrics analysis\n",
        "dex_trades = query_result.copy()\n",
        "\n",
        "# Convert 'block_date' to datetime - it has to be converted again later when setting index because of the dt.date applied first\n",
        "dex_trades['block_date'] = pd.to_datetime(dex_trades['block_date']).dt.date\n",
        "dex_trades = dex_trades.set_index(pd.to_datetime(dex_trades['block_date'])).drop('block_date', axis = 1)\n",
        "\n",
        "# Isolate only the SAFE numbers of the trades - with addresses of trades\n",
        "\n",
        "# SAFE Buys\n",
        "dt_buy = dex_trades.loc[dex_trades['token_bought_symbol'] == 'SAFE'].loc[:, ['blockchain', 'label', 'tx_from', 'token_bought_amount', 'amount_usd']]\n",
        "dt_buy['type'] = 'buy'\n",
        "dt_buy = dt_buy.loc[:, ['blockchain', 'label', 'type', 'tx_from', 'token_bought_amount', 'amount_usd']]\n",
        "dt_buy.columns = ['blockchain', 'label', 'type', 'address', 'amount', 'amount_usd']\n",
        "\n",
        "# SAFE Sells\n",
        "dt_sell = dex_trades.loc[dex_trades['token_sold_symbol'] == 'SAFE'].loc[:, ['blockchain', 'label', 'tx_from', 'token_sold_amount', 'amount_usd']]\n",
        "dt_sell['type'] = 'sell'\n",
        "dt_sell = dt_sell.loc[:, ['blockchain', 'label', 'type', 'tx_from', 'token_sold_amount', 'amount_usd']]\n",
        "dt_sell.columns = ['blockchain', 'label', 'type', 'address', 'amount', 'amount_usd']\n",
        "\n",
        "# Join DFs for complete one - will also be used in the other metrics analysis\n",
        "dt = pd.concat([dt_buy, dt_sell], axis = 0)\n",
        "\n",
        "# ==============================================================================\n",
        "# Ethereum statistics of trades to export to the JT sheet\n",
        "# ==============================================================================\n",
        "\n",
        "# Filter by blockchain and trade size wanting to be filtered (defined below in the trading activity analysis)\n",
        "dt_eth = dt.loc[(dt['blockchain'] == 'ethereum') & (dt['amount'] < 100000)].copy()  # Rationale for 100k SAFE size is explained in Cleaning section below\n",
        "\n",
        "# Get metrics daily\n",
        "aggregations = [\n",
        "    ('trades', 'count'),\n",
        "    ('min', 'min'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('max', 'max'),\n",
        "    ('0.05', lambda x: x.quantile(0.05)),\n",
        "    ('0.90', lambda x: x.quantile(0.90)),\n",
        "    ('0.95', lambda x: x.quantile(0.95)),\n",
        "    ('0.98', lambda x: x.quantile(0.98)),\n",
        "    ('0.99', lambda x: x.quantile(0.99))\n",
        "]\n",
        "\n",
        "dt_eth_daily = dt_eth.resample('D')['amount'].agg(aggregations)  # Remember this has removed the 100k trades\n",
        "\n",
        "# Make sure NaN are treated as 0\n",
        "dt_eth_daily = dt_eth_daily.fillna(0)\n",
        "\n",
        "# Rollsing statistics - the percentiles targetted here depend on the analysis below\n",
        "dt_eth_daily['7d_mean'] = dt_eth_daily['mean'].rolling(window = 7).mean()\n",
        "dt_eth_daily['14d_mean'] = dt_eth_daily['mean'].rolling(window = 14).mean()\n",
        "dt_eth_daily['30d_mean'] = dt_eth_daily['mean'].rolling(window = 30).mean()\n",
        "\n",
        "dt_eth_daily['0.95_30d'] = dt_eth_daily['0.95'].rolling(window = 30).mean()\n",
        "dt_eth_daily['0.98_30d'] = dt_eth_daily['0.98'].rolling(window = 30).mean()\n",
        "dt_eth_daily['0.99_30d'] = dt_eth_daily['0.99'].rolling(window = 30).mean()\n",
        "\n",
        "# Add blockchain\n",
        "dt_eth_daily['blockchain'] = 'ethereum'\n",
        "\n",
        "# Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "dt_eth_daily['etl_dt'] = etl_now\n",
        "dt_eth_daily['etl_dt'] = dt_eth_daily['etl_dt'].dt.floor('s')\n",
        "\n",
        "# ==============================================================================\n",
        "# Gnosis statistics of trades to export to the JT sheet\n",
        "# ==============================================================================\n",
        "\n",
        "# Filter by blockchain and trade size wanting to be filtered (defined below in the trading activity analysis)\n",
        "dt_gno = dt.loc[dt['blockchain'] == 'gnosis'].copy()  # Rationale for 100k SAFE size is explained in Cleaning section below\n",
        "\n",
        "# Descriptive Statistics\n",
        "dt_gno.describe()\n",
        "\n",
        "# Get metrics daily\n",
        "aggregations = [\n",
        "    ('trades', 'count'),\n",
        "    ('min', 'min'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('max', 'max'),\n",
        "    ('0.05', lambda x: x.quantile(0.05)),\n",
        "    ('0.90', lambda x: x.quantile(0.90)),\n",
        "    ('0.95', lambda x: x.quantile(0.95)),\n",
        "    ('0.98', lambda x: x.quantile(0.98)),\n",
        "    ('0.99', lambda x: x.quantile(0.99))\n",
        "]\n",
        "\n",
        "dt_gno_daily = dt_gno.resample('D')['amount'].agg(aggregations)  # Remember this has removed the 100k trades\n",
        "\n",
        "# Make sure NaN are treated as 0\n",
        "dt_gno_daily = dt_gno_daily.fillna(0)\n",
        "\n",
        "# Rollsing statistics - the percentiles targetted here depend on the analysis below\n",
        "dt_gno_daily['7d_mean'] = dt_gno_daily['mean'].rolling(window = 7).mean()\n",
        "dt_gno_daily['14d_mean'] = dt_gno_daily['mean'].rolling(window = 14).mean()\n",
        "dt_gno_daily['30d_mean'] = dt_gno_daily['mean'].rolling(window = 30).mean()\n",
        "\n",
        "dt_gno_daily['0.95_30d'] = dt_gno_daily['0.95'].rolling(window = 30).mean()\n",
        "dt_gno_daily['0.98_30d'] = dt_gno_daily['0.98'].rolling(window = 30).mean()\n",
        "dt_gno_daily['0.99_30d'] = dt_gno_daily['0.99'].rolling(window = 30).mean()\n",
        "\n",
        "# Add blockchain\n",
        "dt_gno_daily['blockchain'] = 'gnosis'\n",
        "\n",
        "# Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "dt_gno_daily['etl_dt'] = etl_now\n",
        "dt_gno_daily['etl_dt'] = dt_gno_daily['etl_dt'].dt.floor('s')\n",
        "\n",
        "# ==============================================================================\n",
        "# Combined Data export\n",
        "# ==============================================================================\n",
        "\n",
        "dex_trades_exp = pd.concat([dt_eth_daily, dt_gno_daily], axis = 0)\n",
        "\n",
        "# Open sheet inside workbook\n",
        "sheet = wb.worksheet('dex_trades')\n",
        "\n",
        "# Reset index for all data to appear - date is index\n",
        "dex_trades_exp = dex_trades_exp.reset_index()\n",
        "\n",
        "# Convert relevant columns in dataframe to str to export - plus additional NaN values to nan in string\n",
        "dex_trades_exp = dex_trades_exp.astype({'block_date': str, 'etl_dt': str})\n",
        "dex_trades_exp = dex_trades_exp.fillna('nan')\n",
        "\n",
        "# DataFrame export\n",
        "dex_trades_exp_export_list = [dex_trades_exp.columns.tolist()]  # Column labels\n",
        "dex_trades_exp_export_list.extend(dex_trades_exp.values.tolist())\n",
        "sheet.update(\"A1\", dex_trades_exp_export_list)\n",
        "\n",
        "# Export to csv\n",
        "dex_trades_exp.to_csv('dex_trades.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aBllUiMYmU0"
      },
      "outputs": [],
      "source": [
        "dex_trades_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkC9ydZcsHOI"
      },
      "outputs": [],
      "source": [
        "dt_eth_daily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXsEDduuEtsX"
      },
      "source": [
        "# Other Metrics Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV7uPCXaDu3f"
      },
      "source": [
        "## SAFE/ETH Max Total Weekly Drawdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_tdL5ijt7K"
      },
      "outputs": [],
      "source": [
        "# Uses total_df from market_data section\n",
        "\n",
        "safe_eth = total_df.loc[total_df['asset'] == 'safe']['close'] / total_df.loc[total_df['asset'] == 'eth']['close']\n",
        "\n",
        "safe_eth_w = safe_eth / safe_eth.shift(7) - 1  # Moves one row downards\n",
        "safe_eth_w.describe()  # Looking for max eth_safe depreciation weekly to put the ranges in the Uni V3 pool - we rebalance weekly so we have to aim to cover max possible deppreciation with desired liquidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE8B0X6Nb3Nw"
      },
      "outputs": [],
      "source": [
        "safe_eth_w_df = safe_eth_w.to_frame()\n",
        "\n",
        "safe_eth_w_df.loc[safe_eth_w_df['close'] == safe_eth_w_df.describe().loc['min','close']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMqYnoapsiYG"
      },
      "source": [
        "## Mainnet DEX Trading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD-0479ooA12"
      },
      "outputs": [],
      "source": [
        "# Uses dex_trades dataframe from dex_trades section\n",
        "\n",
        "# EXTENDED Analysis of ETHEREUM trades\n",
        "\n",
        "dt_eth_ext = dt.loc[dt['blockchain'] == 'ethereum']\n",
        "\n",
        "dt_eth_ext.describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcTTWsC6x8PM"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of trades to see if there are outliers (I want to target liquidity based on normal data)\n",
        "\n",
        "# Creating 2x2 subplots with histograms for each column\n",
        "# fig, axs = plt.subplots(1, 2, figsize=(14, 8))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_axes(rect = [1, 1, 1, 1])\n",
        "\n",
        "ax.hist(dt_eth_ext['amount'], bins=150, edgecolor='black')\n",
        "ax.set_title('Trade Size Distribution')\n",
        "ax.set_xlabel('Trade Size')\n",
        "ax.set_ylabel('Number Of Trades')\n",
        "\n",
        "# Set tick sizes to see better\n",
        "ticks = [x for x in range(0, int(dt_eth_ext['amount'].max()), 10000)]\n",
        "ax.set_xticks(ticks, labels=None)  # Se pasan los ticks en una lista/array\n",
        "ax.tick_params('x', labelrotation = 45)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cM5RP431Y5w"
      },
      "outputs": [],
      "source": [
        "# Since it's a time series we can also check the average trade size for particular dates\n",
        "\n",
        "dt_eth_ext_daily = dt_eth_ext.resample('D')['amount'].agg(['mean', 'median', 'max'])\n",
        "\n",
        "dt_eth_ext_daily[['mean', 'median', 'max']].plot(figsize=(12,6))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDQNh6Ky5DP8"
      },
      "source": [
        "#### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tur9zbIZvo2-"
      },
      "outputs": [],
      "source": [
        "# Clean the dt_ext\n",
        "\n",
        "# Take out the outliers seen above\n",
        "\n",
        "# There seems to be only four dates with a maximum above 100,000 SAFE\n",
        "pprint.pp(dt_eth_ext_daily.loc[dt_eth_ext_daily['max'] > 100000].index.unique().values)\n",
        "print('')\n",
        "dt_eth_ext_daily.loc[dt_eth_ext_daily['max'] < 100000].plot(figsize=(12,6))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2RU2cdM5CHW"
      },
      "outputs": [],
      "source": [
        "# What happens if we only take out the trades above that size\n",
        "\n",
        "# As a reminder - We want to target 30d mean and the surplus from there to either the 0.95, 0.98, 0.99 rolling 30d mean as well\n",
        "\n",
        "dt_ = dt.loc[dt['blockchain'] == 'ethereum'].copy()\n",
        "\n",
        "\n",
        "print('Amount of trades above 100k SAFE:', dt_.loc[dt_['amount'] > 100000].size)\n",
        "print('Out of a total of:', dt_.size)\n",
        "print('Trades >100k SAFE over total Trades:', f\"{dt_.loc[dt_['amount'] > 100000].size / dt.size * 100}%\")\n",
        "\n",
        "print('')\n",
        "print('Statistics without the outliers')\n",
        "dt_.loc[dt_['amount'] < 100000].describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2vU89GQs1K-"
      },
      "outputs": [],
      "source": [
        "# Addresses per trading bracket\n",
        "\n",
        "# Define the percentiles we are interested in\n",
        "percentiles = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "# Calculate the percentile values for amount\n",
        "percentile_values = dt_eth_ext['amount'].quantile(percentiles)\n",
        "\n",
        "# Dictionary to hold the unique addresses for each percentile range\n",
        "unique_addresses_by_percentile = {}\n",
        "\n",
        "# For each percentile, filter the dataframe and find unique 'label'\n",
        "for p in percentiles:\n",
        "    threshold = percentile_values[p]\n",
        "    filtered_df = dt_eth_ext.loc[dt_eth_ext['amount'] >= threshold]\n",
        "    unique_addresses = filtered_df['address'].nunique()\n",
        "    unique_addresses_by_percentile[f'{int(p*100)}th Percentile'] = unique_addresses\n",
        "\n",
        "# Convert the results into a DataFrame for better readability\n",
        "unique_addresses_df = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in unique_addresses_by_percentile.items()])).T\n",
        "\n",
        "unique_addresses_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOy96bG-ovr3"
      },
      "source": [
        "### More Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjn6KsPDo0st"
      },
      "outputs": [],
      "source": [
        "dt_eth_ext.resample('D').mean(numeric_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYZ5PSY0p1XA"
      },
      "outputs": [],
      "source": [
        "\"\"\"Total size and value traded\n",
        "\"\"\"\n",
        "\n",
        "dt_eth_ext.resample('D').sum(numeric_only = True).loc['2024-07':].plot(figsize=(12,6))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoY9_uhsR1Fs"
      },
      "source": [
        "## Gnosis Chain DEX Trading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E98VWhxaUrHt"
      },
      "outputs": [],
      "source": [
        "# Uses dex_trades dataframe from dex_trades section\n",
        "\n",
        "# EXTENDED Analysis of ETHEREUM trades\n",
        "\n",
        "dt_gno_ext = dt.loc[dt['blockchain'] == 'gnosis']\n",
        "\n",
        "dt_gno_ext.describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRQdhVLlUznf"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of trades to see if there are outliers (I want to target liquidity based on normal data)\n",
        "\n",
        "# Creating 2x2 subplots with histograms for each column\n",
        "# fig, axs = plt.subplots(1, 2, figsize=(14, 8))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_axes(rect = [1, 1, 1, 1])\n",
        "\n",
        "ax.hist(dt_gno_ext['amount'], bins=150, edgecolor='black')\n",
        "ax.set_title('Trade Size Distribution')\n",
        "ax.set_xlabel('Trade Size')\n",
        "ax.set_ylabel('Number Of Trades')\n",
        "\n",
        "# Set tick sizes to see better\n",
        "ticks = [x for x in range(0, int(dt_gno_ext['amount'].max()), 500)]\n",
        "ax.set_xticks(ticks, labels=None)  # Se pasan los ticks en una lista/array\n",
        "ax.tick_params('x', labelrotation = 45)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWL1iFEuVq8p"
      },
      "outputs": [],
      "source": [
        "# Since it's a time series we can also check the average trade size for particular dates\n",
        "\n",
        "dt_gno_ext_daily = dt_gno_ext.resample('D')['amount'].agg(['mean', 'median', 'max'])\n",
        "\n",
        "dt_gno_ext_daily[['mean', 'median', 'max']].plot(figsize=(12,6))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KQxGxlTWEoD"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeVW-u-lVxgI"
      },
      "outputs": [],
      "source": [
        "# Clean the dt_ext\n",
        "\n",
        "# Take out the outliers seen above\n",
        "\n",
        "# There seems to be only four dates with a maximum above 100,000 SAFE\n",
        "pprint.pp(dt_gno_ext_daily.loc[dt_gno_ext_daily['max'] > 8000].index.unique().values)\n",
        "print('')\n",
        "dt_gno_ext_daily.loc[dt_gno_ext_daily['max'] < 8000].plot(figsize=(12,6))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhnVYR8uWHiA"
      },
      "outputs": [],
      "source": [
        "# What happens if we only take out the trades above that size\n",
        "\n",
        "# As a reminder - We want to target 30d mean and the surplus from there to either the 0.95, 0.98, 0.99 rolling 30d mean as well\n",
        "\n",
        "dt_ = dt.loc[dt['blockchain'] == 'gnosis'].copy()\n",
        "\n",
        "\n",
        "print('Amount of trades above 8k SAFE:', dt_.loc[dt_['amount'] > 8000].size)\n",
        "print('Out of a total of:', dt_.size)\n",
        "print('Trades >8k SAFE over total Trades:', f\"{dt_.loc[dt_['amount'] > 8000].size / dt.size * 100}%\")\n",
        "\n",
        "print('')\n",
        "print('Statistics without the outliers')\n",
        "dt_.loc[dt_['amount'] < 8000].describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5U9FyC6Wblr"
      },
      "source": [
        "## Extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdiNn2wNWcvo"
      },
      "outputs": [],
      "source": [
        "dt.loc[dt['amount'] > 10000].loc['2024-09-17':]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAu2XmxIFz6-"
      },
      "source": [
        "# Close Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJF3Bu5zF37w"
      },
      "outputs": [],
      "source": [
        "# wb.client.session.close()  # Close the session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbqLWAp_-OLN"
      },
      "source": [
        "# Security Backup (WIP)\n",
        "\n",
        "Create a csv file backup in Drive for each table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMb9gawLDmgT"
      },
      "source": [
        "# Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn1ydTf7Dn6p"
      },
      "outputs": [],
      "source": [
        "# # ==============================================================================\n",
        "# # SAFE DEX Data from Dune Analytics\n",
        "# # ==============================================================================\n",
        "\n",
        "# query_id = 3777854  # SAFE - * - GMD - $SAFE DEX Trading Volume (Pool)\n",
        "\n",
        "# headers = {'X-DUNE-API-KEY': config['api_keys']['dune']['key']}\n",
        "\n",
        "# delay = 10  # time delay in seconds for checking succesfull execution\n",
        "# q = 0\n",
        "\n",
        "# # Request for executing the query\n",
        "\n",
        "# base_url = f\"https://api.dune.com/api/v1/query/{query_id}/execute\"\n",
        "\n",
        "# try:\n",
        "#     r = requests.post(base_url, headers=headers)\n",
        "#     r.raise_for_status()  # Raise an exception for unsuccessful HTTP status codes\n",
        "\n",
        "#     execution_id = r.json()['execution_id']  # Get exec id for status check\n",
        "#     time.sleep(1)\n",
        "\n",
        "#     print(\"Request successful!\")\n",
        "# except requests.exceptions.RequestException as e:\n",
        "#     print(\"An error occurred:\", e)\n",
        "\n",
        "# # Checking the query execution status\n",
        "\n",
        "# base_url = f\"https://api.dune.com/api/v1/execution/{execution_id}/status\"\n",
        "\n",
        "# while True:\n",
        "#   r = requests.get(base_url, headers=headers)\n",
        "\n",
        "#   if r.json()['is_execution_finished']:\n",
        "#     print(\"Execution Finished\")\n",
        "#     break\n",
        "\n",
        "#   print(\"Awating Execution:\", str(q), 's')\n",
        "#   time.sleep(delay)\n",
        "#   q += delay\n",
        "\n",
        "# # Getting the query results - with Dune Client\n",
        "\n",
        "# dune = DuneClient(\n",
        "#     api_key = config['api_keys']['dune']['key'],\n",
        "#     base_url = \"https://api.dune.com\",\n",
        "#     request_timeout=(300) # request will time out after 300 seconds\n",
        "# )\n",
        "\n",
        "# query_result = dune.get_latest_result_dataframe(\n",
        "#     query = query_id\n",
        "#     # # filter for users account more than a month old and more than bottom active tier\n",
        "#     # , filters=\"account_age > 30 and fid_active_tier > 1\"\n",
        "#     # # sort result by number of channels they are follow in descending order\n",
        "#     # , sort_by=[\"channels desc\"]\n",
        "# )\n",
        "\n",
        "# # Make date index to resample and SUM data\n",
        "# query_result['date'] = pd.to_datetime(query_result['block_date'])\n",
        "\n",
        "# # Daily aggregation of numeric data\n",
        "# dex_df = query_result.resample('D', kind = 'period', on = 'date').sum(numeric_only = True)\n",
        "\n",
        "# # Select & Rename columns\n",
        "# dex_df = dex_df[['trades', 'total_vol']]\n",
        "# dex_df.columns = ['dex_trades', 'dex_vol']\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Joining CG + DEX Data & final things\n",
        "# # ==============================================================================\n",
        "\n",
        "# # Add asset col for joining\n",
        "# dex_df['asset'] = 'safe'\n",
        "\n",
        "# dex_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMIrVaW4ldpJ"
      },
      "source": [
        "## CoW AMM\n",
        "\n",
        "- https://docs.cow.fi/cow-protocol/reference/apis/orderbook\n",
        "- https://explorer.cow.fi/address/0x027e1cbf2c299cba5eb8a2584910d04f1a8aa403\n",
        "\n",
        "- https://colab.research.google.com/drive/1Si3tBKl004UVxGz9I9spEMUJpc7IMkZs?usp=sharing#scrollTo=8f3iYPODSXbP\n",
        "- https://colab.research.google.com/drive/1UKUNSMFoOLeg9fy1v-TYnymHSn7N1ghq?usp=sharing#scrollTo=8f3iYPODSXbP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KrRlRNXmb2w"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import pandas as pd\n",
        "\n",
        "# \"\"\"\n",
        "# Get CoW AMM orders (executed and failed)\n",
        "# \"\"\"\n",
        "\n",
        "# account_address = '0x027e1cbf2c299cba5eb8a2584910d04f1a8aa403'\n",
        "\n",
        "# base_url = f\"https://api.cow.fi/mainnet/api/v1/account/{account_address}/orders\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

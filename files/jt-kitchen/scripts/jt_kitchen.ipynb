{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLo1BmaH03uY"
      },
      "source": [
        "# intro & instructions\n",
        "\n",
        "Jupyter notebook for data extraction and processing for Joint Treasury data analysis. **Execution is on Colab** (not locally).\n",
        "\n",
        "Sections:\n",
        "1. **setup:** done in the first section in order to have proper config for the whole nobtebook.\n",
        "2. **data collection:** section used for collecting data for the jt kitchen.\n",
        "    1. **prices:** fetch prices for JT portfolio relevant tokens.\n",
        "    2. **tickers:** ...\n",
        "3. **data analysis:** section used for analysis of data outside of the jt kitchen."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vV5TLZHzTSl3"
      },
      "source": [
        "# setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "35zmnTpcw1Lu",
        "outputId": "a3ba78f2-0239-49f1-9a2b-3db674615718",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://****@github.com/tom4s-lt/kpk-kitchens.git@main\n",
            "  Cloning https://****@github.com/tom4s-lt/kpk-kitchens.git (to revision main) to /tmp/pip-req-build-f94h3a2e\n",
            "  Running command git clone --filter=blob:none --quiet 'https://****@github.com/tom4s-lt/kpk-kitchens.git' /tmp/pip-req-build-f94h3a2e\n",
            "  Resolved https://****@github.com/tom4s-lt/kpk-kitchens.git to commit 578224157ad37bd2b0dc5703258b90fd0ce89874\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from kpk_kitchens==0.1.1) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kpk_kitchens==0.1.1) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from kpk_kitchens==0.1.1) (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (from kpk_kitchens==0.1.1) (0.13.2)\n",
            "Requirement already satisfied: gspread in /usr/local/lib/python3.11/dist-packages (from kpk_kitchens==0.1.1) (6.2.1)\n",
            "Collecting dune_client (from kpk_kitchens==0.1.1)\n",
            "  Downloading dune_client-1.7.10-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting dune_spice (from kpk_kitchens==0.1.1)\n",
            "  Downloading dune_spice-0.2.6-py3-none-any.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (from kpk_kitchens==0.1.1) (1.21.0)\n",
            "Requirement already satisfied: aiohttp>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from dune_client->kpk_kitchens==0.1.1) (3.11.15)\n",
            "Collecting dataclasses-json>=0.6.4 (from dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting types-python-dateutil>=2.8.19 (from dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading types_python_dateutil-2.9.0.20250708-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting types-PyYAML>=6.0.11 (from dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading types_pyyaml-6.0.12.20250516-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting types-requests>=2.28.0 (from dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading types_requests-2.32.4.20250611-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting types-Deprecated>=1.2.9.3 (from dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading types_deprecated-1.2.15.20250304-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: types-setuptools>=68.2.0.0 in /usr/local/lib/python3.11/dist-packages (from dune_client->kpk_kitchens==0.1.1) (80.9.0.20250529)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from dune_client->kpk_kitchens==0.1.1) (2.9.0.post0)\n",
            "Collecting ndjson>=0.3.1 (from dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading ndjson-0.3.1-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Deprecated>=1.2.0 (from dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kpk_kitchens==0.1.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kpk_kitchens==0.1.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kpk_kitchens==0.1.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kpk_kitchens==0.1.1) (2025.7.9)\n",
            "Requirement already satisfied: rich>=13.3.3 in /usr/local/lib/python3.11/dist-packages (from dune_spice->kpk_kitchens==0.1.1) (13.9.4)\n",
            "Collecting rich-argparse>=1.5.2 (from dune_spice->kpk_kitchens==0.1.1)\n",
            "  Downloading rich_argparse-1.7.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: google-auth>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from gspread->kpk_kitchens==0.1.1) (2.38.0)\n",
            "Requirement already satisfied: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from gspread->kpk_kitchens==0.1.1) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->kpk_kitchens==0.1.1) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->kpk_kitchens==0.1.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->kpk_kitchens==0.1.1) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (1.20.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json>=0.6.4->dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json>=0.6.4->dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated>=1.2.0->dune_client->kpk_kitchens==0.1.1) (1.17.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread->kpk_kitchens==0.1.1) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread->kpk_kitchens==0.1.1) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.12.0->gspread->kpk_kitchens==0.1.1) (4.9.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib>=0.4.1->gspread->kpk_kitchens==0.1.1) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->dune_client->kpk_kitchens==0.1.1) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.3->dune_spice->kpk_kitchens==0.1.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.3->dune_spice->kpk_kitchens==0.1.1) (2.19.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp>=3.8.3->dune_client->kpk_kitchens==0.1.1) (4.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.3->dune_spice->kpk_kitchens==0.1.1) (0.1.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread->kpk_kitchens==0.1.1) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread->kpk_kitchens==0.1.1) (3.3.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json>=0.6.4->dune_client->kpk_kitchens==0.1.1)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Downloading dune_client-1.7.10-py3-none-any.whl (37 kB)\n",
            "Downloading dune_spice-0.2.6-py3-none-any.whl (23 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
            "Downloading ndjson-0.3.1-py2.py3-none-any.whl (5.3 kB)\n",
            "Downloading rich_argparse-1.7.1-py3-none-any.whl (25 kB)\n",
            "Downloading types_deprecated-1.2.15.20250304-py3-none-any.whl (8.6 kB)\n",
            "Downloading types_python_dateutil-2.9.0.20250708-py3-none-any.whl (17 kB)\n",
            "Downloading types_pyyaml-6.0.12.20250516-py3-none-any.whl (20 kB)\n",
            "Downloading types_requests-2.32.4.20250611-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Building wheels for collected packages: kpk_kitchens\n",
            "  Building wheel for kpk_kitchens (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kpk_kitchens: filename=kpk_kitchens-0.1.1-py3-none-any.whl size=5574 sha256=e144713d77b670aa28db21c2fe2904c7d112a165346f20b2309aff57ae224daa\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p2k0cnlx/wheels/70/78/1f/a3817b10f7b263cc997f52149699653337638b01717388c103\n",
            "Successfully built kpk_kitchens\n",
            "Installing collected packages: ndjson, types-requests, types-PyYAML, types-python-dateutil, types-Deprecated, mypy-extensions, marshmallow, Deprecated, typing-inspect, rich-argparse, dataclasses-json, dune_spice, dune_client, kpk_kitchens\n",
            "Successfully installed Deprecated-1.2.18 dataclasses-json-0.6.7 dune_client-1.7.10 dune_spice-0.2.6 kpk_kitchens-0.1.1 marshmallow-3.26.1 mypy-extensions-1.1.0 ndjson-0.3.1 rich-argparse-1.7.1 types-Deprecated-1.2.15.20250304 types-PyYAML-6.0.12.20250516 types-python-dateutil-2.9.0.20250708 types-requests-2.32.4.20250611 typing-inspect-0.9.0\n",
            "Requirement already satisfied: dune_spice in /usr/local/lib/python3.11/dist-packages (0.2.6)\n",
            "Requirement already satisfied: polars>=1.0 in /usr/local/lib/python3.11/dist-packages (from dune_spice) (1.21.0)\n",
            "Requirement already satisfied: requests>=2.16 in /usr/local/lib/python3.11/dist-packages (from dune_spice) (2.32.3)\n",
            "Requirement already satisfied: aiohttp>=3.9.5 in /usr/local/lib/python3.11/dist-packages (from dune_spice) (3.11.15)\n",
            "Requirement already satisfied: rich>=13.3.3 in /usr/local/lib/python3.11/dist-packages (from dune_spice) (13.9.4)\n",
            "Requirement already satisfied: rich-argparse>=1.5.2 in /usr/local/lib/python3.11/dist-packages (from dune_spice) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.5->dune_spice) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.5->dune_spice) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.5->dune_spice) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.5->dune_spice) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.5->dune_spice) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.5->dune_spice) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.9.5->dune_spice) (1.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16->dune_spice) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16->dune_spice) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16->dune_spice) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.16->dune_spice) (2025.7.9)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.3->dune_spice) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.3.3->dune_spice) (2.19.2)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.1.2->aiohttp>=3.9.5->dune_spice) (4.14.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.3->dune_spice) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Setup all the required variables & logic for the notebook.\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================\n",
        "#  Install Google Auth Libraries + auth itself (go first to click on pop up quickly)\n",
        "# ==============================================\n",
        "\n",
        "# Google authentication libraries\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "import gspread\n",
        "from google.auth import default\n",
        "\n",
        "# ==============================================\n",
        "#  Install Required Packages\n",
        "# ==============================================\n",
        "\n",
        "# user-built packages to run in the colab\n",
        "GITHUB_TOKEN = \"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\"\n",
        "BRANCH = \"main\"\n",
        "! pip install git+https://{GITHUB_TOKEN}@github.com/tom4s-lt/kpk-kitchens.git@{BRANCH}\n",
        "\n",
        "! pip install dune_spice\n",
        "\n",
        "# ==============================================\n",
        "#  Import Required Libraries\n",
        "# ==============================================\n",
        "\n",
        "# user-built config class and functions\n",
        "from kpk_kitchens.config import JTConfig\n",
        "from kpk_kitchens.utils import etl_gen_df_from_gsheet, gecko_get_price_historical\n",
        "\n",
        "# Other libraries\n",
        "import os\n",
        "import requests\n",
        "import pandas as pd\n",
        "import polars\n",
        "\n",
        "import spice\n",
        "\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "from typing import Optional, Dict, Any, List\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pprint\n",
        "\n",
        "# ==============================================\n",
        "#  Initialize script params\n",
        "# ==============================================\n",
        "\n",
        "# Google credentials\n",
        "creds, _ = default()\n",
        "gc = gspread.authorize(creds)\n",
        "\n",
        "# Create the data directory\n",
        "os.makedirs(JTConfig.DATA_DIR, exist_ok=True)\n",
        "\n",
        "# Open workbook\n",
        "wb = gc.open_by_url(JTConfig.WORKBOOK_URL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3zzLKCxlRS3"
      },
      "source": [
        "# data collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9Bmwgx1dPV9"
      },
      "source": [
        "Extracting and handling data to be exported to the JT Sheet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyZ-vy7gWKpi"
      },
      "source": [
        "## prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "YzBtUTajWKQQ",
        "outputId": "2e84d373-156d-4659-b396-0fa77265ea56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 0 stablecoins and 0 non-stablecoins\n",
            "\n",
            "Price data collection complete\n",
            "\n",
            "Processing price data...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects to concatenate",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-236202169.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Process price data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nProcessing price data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mdf_prices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprice_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0mdf_prices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_prices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ms'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m     op = _Concatenator(\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_keys_and_objs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;31m# figure out what our result ndim is going to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m_clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Fetches prices for ens portfolio relevant tokens from CoinGecko.\n",
        "\n",
        "Most failures come from duplicate assets in the asset tab.\n",
        "\"\"\"\n",
        "\n",
        "# Fetch assets from Google Sheet\n",
        "json_lk_assets = etl_gen_df_from_gsheet(gc, JTConfig.WORKBOOK_URL, JTConfig.LK_ASSETS_TAB)\n",
        "\n",
        "# Separate stablecoins and non-stablecoins\n",
        "stablecoins = [\n",
        "    asset for asset in json_lk_assets\n",
        "    if asset.get(\"gecko_id\", \"\") and asset.get(\"stablecoin\", False)\n",
        "]\n",
        "\n",
        "non_stablecoins = [\n",
        "    asset for asset in json_lk_assets\n",
        "    if asset.get(\"gecko_id\", \"\") and not asset.get(\"stablecoin\", False)\n",
        "]\n",
        "\n",
        "print(f\"Found {len(stablecoins)} stablecoins and {len(non_stablecoins)} non-stablecoins\")\n",
        "\n",
        "# Fetch and process price data for non-stablecoin assets\n",
        "price_data = []\n",
        "for asset in non_stablecoins:\n",
        "    print(f\"Fetching data for {asset['symbol']}...\")\n",
        "\n",
        "    gecko_hist_data = gecko_get_price_historical(\n",
        "        base_url=JTConfig.COINGECKO_API_BASE_URL,\n",
        "        asset_id=asset['gecko_id'],\n",
        "        api_key=JTConfig.COINGECKO_API_KEY,\n",
        "        max_retries=JTConfig.MAX_RETRIES,\n",
        "        retry_delay=JTConfig.RETRY_DELAY,\n",
        "        timeout=JTConfig.DEFAULT_TIMEOUT,\n",
        "        # params is function default - 365 days max with free key\n",
        "        headers={\n",
        "            'accept': 'application/json',\n",
        "            'x-cg-demo-api-key': JTConfig.COINGECKO_API_KEY\n",
        "        }\n",
        "    )\n",
        "\n",
        "    if gecko_hist_data:\n",
        "        # Create DataFrame for current asset\n",
        "        df = pd.DataFrame(gecko_hist_data['prices'], columns=['ts', 'price'])\n",
        "        df['gecko_id'] = asset['gecko_id']\n",
        "        df['symbol'] = asset['symbol']\n",
        "        price_data.append(df)\n",
        "        print(f\"Successfully fetched data for {asset['symbol']}\")\n",
        "\n",
        "    time.sleep(3)  # Rate limiting\n",
        "\n",
        "print(\"\\nPrice data collection complete\")\n",
        "\n",
        "# Process price data\n",
        "print(\"\\nProcessing price data...\")\n",
        "df_prices = pd.concat(price_data)\n",
        "df_prices['date'] = pd.to_datetime(df_prices['ts'], unit='ms')\n",
        "\n",
        "# Resample to daily frequency and calculate mean prices - also eliminates duplicates\n",
        "df_prices = (df_prices\n",
        "    .groupby(['symbol', 'gecko_id'])\n",
        "    .resample('D', on='date')\n",
        "    .mean()\n",
        "    .reset_index()\n",
        "    [['date', 'symbol', 'gecko_id', 'price']]  # Drop ts\n",
        "    .sort_values('date', ascending=False)\n",
        ")\n",
        "\n",
        "print(\"\\nPrice data processing complete\")\n",
        "\n",
        "# Add stablecoin data with price=1\n",
        "if stablecoins:\n",
        "    print(\"\\nAdding stablecoin data...\")\n",
        "    # Get unique dates from the price data\n",
        "    dates = df_prices['date'].unique()\n",
        "\n",
        "    # Create stablecoin records\n",
        "    stablecoin_data = []\n",
        "    for asset in stablecoins:\n",
        "        for date in dates:\n",
        "            stablecoin_data.append({\n",
        "                'date': date,\n",
        "                'symbol': asset['symbol'],\n",
        "                'gecko_id': asset['gecko_id'],\n",
        "                'price': 1.0\n",
        "            })\n",
        "\n",
        "    # Convert to DataFrame and append to price data\n",
        "    df_stablecoins = pd.DataFrame(stablecoin_data)\n",
        "    df_prices = pd.concat([df_prices, df_stablecoins], ignore_index=True)\n",
        "    df_prices = df_prices.sort_values('date', ascending=False)\n",
        "\n",
        "print(\"\\nStablecoin prices complete\")\n",
        "\n",
        "# Export results\n",
        "print(f\"\\nExporting results to {JTConfig.PRICES_CSV}\")\n",
        "df_prices.to_csv(JTConfig.PRICES_CSV, index=False)\n",
        "print(\"\\nExport complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdox00HD3FTY"
      },
      "source": [
        "## CG - tickers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "9bfPevAb3Kqh",
        "outputId": "7ca342af-90f4-4b1d-9d75-e96c50a79dc8"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'Config' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3560510634.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;31m# Get current data from the tab (it's a timeseries that gets updated with latest data) - plus convert imported datatypes (only the relevant ones)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mpast_tickers_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0metl_gen_df_from_gsheet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwb_url\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWORKBOOK_URL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tickers'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'df'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;31m# Convert data from all rows that should be numeric to numeric - convert to np.nan if impossible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Seeks current market data form CoinGecko - the table below each asset on their respective page\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction\n",
        "# ==============================================================================\n",
        "\n",
        "asset = 'safe'\n",
        "\n",
        "endpoint = f\"{JTConfig.COINGECKO_API_BASE_URL}/coins/{asset}/tickers?depth=true\"\n",
        "\n",
        "params = {\n",
        "    'x_cg_demo_api_key': JTConfig.COINGECKO_API_KEY\n",
        "}\n",
        "\n",
        "r = requests.get(endpoint, params=params)\n",
        "\n",
        "# ==============================================================================\n",
        "# Request handling\n",
        "# ==============================================================================\n",
        "\n",
        "tickers = r.json()['tickers']\n",
        "\n",
        "data = []\n",
        "\n",
        "for ticker in range(len(tickers)):\n",
        "    data_temp = {\n",
        "        'exchange': tickers[ticker]['market']['name'],\n",
        "        'pair': tickers[ticker]['base'] + '-' + tickers[ticker]['target'],\n",
        "        'price': tickers[ticker]['last'],\n",
        "        'spread': tickers[ticker]['bid_ask_spread_percentage'],  # it's in percentage, have to pass to decimal\n",
        "        '2_pct': tickers[ticker]['cost_to_move_up_usd'],\n",
        "        '-2_pct': tickers[ticker]['cost_to_move_down_usd'],\n",
        "        '24h_vol': tickers[ticker]['converted_volume']['usd'],\n",
        "        'trust_score': tickers[ticker]['trust_score']\n",
        "    }\n",
        "    data.append(data_temp)\n",
        "\n",
        "df = pd.DataFrame.from_records(data)\n",
        "\n",
        "# Extract blockchain from DEXs by extracting it from the Exchange name\n",
        "df['chain'] = df['exchange'].str.extract(r'\\((.*?)\\)', expand=False)\n",
        "\n",
        "# Add tye of exchange (CEX/DEX)\n",
        "df['type'] = df['chain'].apply(lambda x: 'DEX' if pd.notnull(x) else 'CEX')\n",
        "\n",
        "# Add Volume percentage for each Pair\n",
        "df['vol_pct'] = df['24h_vol'].div(df['24h_vol'].sum())\n",
        "\n",
        "# Add datetime + etl_dt (etl datetime for reference of when info was fetched)\n",
        "df['datetime'] = JTConfig.ETL_NOW\n",
        "df['datetime'] = df['datetime'].dt.floor('s')\n",
        "df['etl_dt'] = df['datetime']\n",
        "\n",
        "# Add vol*-2_pct - for volume weighted -2% size\n",
        "df['depth_*_vol'] = df['-2_pct'] * df['24h_vol']\n",
        "\n",
        "# Add Notes column to match the JT Sheet table that has notes\n",
        "df['notes'] = \"\"\n",
        "\n",
        "# Replace anomally signs in the 'pair' column\n",
        "df['pair'] = df['pair'].str.replace('$', '')\n",
        "df['pair'] = df['pair'].str.replace('SAFE1', 'SAFE')\n",
        "\n",
        "# Replace token addresses\n",
        "df['pair'] = df['pair'].replace(regex=JTConfig.TOKEN_ADDRESSES)\n",
        "\n",
        "# Replace NaN for \"nan\" for correct data for exporting to the Sheet - the same string that is fetched from the import\n",
        "df['chain'] = df['chain'].fillna('nan')\n",
        "\n",
        "# Order as the sheets table - it's to append below\n",
        "df = df[['exchange', 'type', 'pair', 'price', 'spread', '2_pct', '-2_pct', '24h_vol', 'vol_pct', 'trust_score', 'chain', 'depth_*_vol', 'datetime', 'etl_dt', 'notes']]\n",
        "\n",
        "# ==============================================================================\n",
        "# Data export\n",
        "# ==============================================================================\n",
        "\n",
        "tickers_df = df.copy()\n",
        "\n",
        "# Open sheet inside workbook\n",
        "sheet = wb.worksheet('tickers')\n",
        "\n",
        "# Convert dateitme in dataframe to str to export\n",
        "tickers_df = tickers_df.astype({'datetime': str, 'etl_dt': str})\n",
        "\n",
        "# Reset index for all data to appear\n",
        "tickers_df = tickers_df.reset_index()\n",
        "\n",
        "# Get current data from the tab (it's a timeseries that gets updated with latest data) - plus convert imported datatypes (only the relevant ones)\n",
        "past_tickers_df = etl_gen_df_from_gsheet(wb_url=JTConfig.WORKBOOK_URL, page='tickers', output_type='df')\n",
        "\n",
        "# Convert data from all rows that should be numeric to numeric - convert to np.nan if impossible\n",
        "past_tickers_df['spread'] = pd.to_numeric(past_tickers_df['spread'], errors='coerce')\n",
        "past_tickers_df['2_pct'] = pd.to_numeric(past_tickers_df['2_pct'], errors='coerce')\n",
        "past_tickers_df['vol_pct'] = pd.to_numeric(past_tickers_df['vol_pct'], errors='coerce')\n",
        "\n",
        "# Convert all columns to their respective dtype for appending the new information\n",
        "past_tickers_df = past_tickers_df.astype({\n",
        "    'price': float,\n",
        "    'spread': float,\n",
        "    '2_pct': float,\n",
        "    '2_pct': float,\n",
        "    '24h_vol': float,\n",
        "    'vol_pct': float,\n",
        "    'depth_*_vol': float\n",
        "})\n",
        "\n",
        "# Join the historical and new dataframes\n",
        "tickers_df = pd.concat([past_tickers_df, tickers_df], axis=0)  # Generate records dataframe to export\n",
        "\n",
        "# Convert relevant columns in dataframe to str to export\n",
        "tickers_df = tickers_df.astype({'spread': str, '2_pct': str, 'vol_pct': str, 'trust_score': str, 'chain': str, 'datetime': str, 'etl_dt': str})\n",
        "\n",
        "# DataFrame export\n",
        "tickers_df_export_list = [tickers_df.columns.tolist()]  # Column labels\n",
        "tickers_df_export_list.extend(tickers_df.values.tolist())\n",
        "sheet.update(\"A1\", tickers_df_export_list)\n",
        "\n",
        "# Export to csv\n",
        "tickers_df.to_csv(f\"{JTConfig.DATA_DIR}/tickers.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtV2goMkeaMD"
      },
      "source": [
        "## CG & Dune - market_data\n",
        "\n",
        "- this can be optimized with a dune extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "collapsed": true,
        "id": "b32FWoGtcU3n",
        "outputId": "2ab0e897-1234-4020-ac0b-cab5952ff8b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-18-307108383.py:137: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  sheet.update(\"A1\", market_data_df_export_list)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Seeks SAFE + similar tokens market data in CoinGecko (since the TTE)\n",
        "Also SAFE DEX Volume to understand the share of trade in each\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "# CoinGecko Call\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction & handling\n",
        "# ==============================================================================\n",
        "\n",
        "# Total number of days to look for market data (from tte to today)\n",
        "days = datetime.today() - datetime.strptime(JTConfig.TTE_DATE, \"%Y-%m-%d\")\n",
        "\n",
        "data = []\n",
        "\n",
        "for id in JTConfig.PORTFOLIO_TOKENS['similar_tokens']:\n",
        "    endpoint = f\"{JTConfig.COINGECKO_API_BASE_URL}/coins/{JTConfig.PORTFOLIO_TOKENS['similar_tokens'][id]}/market_chart?\"\n",
        "    params = {\n",
        "        'vs_currency': 'usd',\n",
        "        'days': 365,  # Then for the shift() method to leave info on first day and have a buffer\n",
        "        'interval': 'daily',\n",
        "        'x_cg_demo_api_key': JTConfig.COINGECKO_API_KEY\n",
        "    }\n",
        "\n",
        "    r = requests.get(endpoint, params=params)\n",
        "\n",
        "    # Process price data\n",
        "    price = pd.DataFrame(r.json()['prices'], columns=['date', 'price'])  # Opening price for the day (first day is not standard)\n",
        "    price['date'] = pd.to_datetime(price['date'], unit='ms')\n",
        "    price = price.set_index('date')\n",
        "    price = price.resample('D', kind='period').last()\n",
        "\n",
        "    # Process market cap data\n",
        "    mcap = pd.DataFrame(r.json()['market_caps'], columns=['date', 'mcap'])  # Mcap for that price\n",
        "    mcap['date'] = pd.to_datetime(mcap['date'], unit='ms')\n",
        "    mcap = mcap.set_index('date')\n",
        "    mcap = mcap.resample('D', kind = 'period').last()\n",
        "\n",
        "    # Process volume data\n",
        "    volume = pd.DataFrame(r.json()['total_volumes'], columns = ['date', 'volume'])  # Volume of the last day\n",
        "    volume['date'] = pd.to_datetime(volume['date'], unit = 'ms')\n",
        "    volume = volume.set_index('date')\n",
        "    volume = volume.resample('D', kind = 'period').last()\n",
        "\n",
        "    # Combine all data & generate asset attribute\n",
        "    df = pd.concat([price, mcap, volume], axis = 'columns')\n",
        "    df['asset'] = id\n",
        "\n",
        "    # Adjust data to reflect the data consistently -  values for each day are actualy the finishing for the earlier one\n",
        "    df.columns = ['open', 'mcap', 'volume', 'asset']\n",
        "    df['volume'] = df['volume'].shift(-1)\n",
        "    df['close'] = df['open'].shift(-1)\n",
        "    df['mcap'] = df['mcap'].shift(-1)\n",
        "\n",
        "    # Organize & slice for dates since TTE\n",
        "    df = df[['asset', 'open', 'close', 'mcap', 'volume']]\n",
        "\n",
        "    # Missing today's close so row is removed\n",
        "    df = df.loc[JTConfig.TTE_DATE:].iloc[:-1]\n",
        "\n",
        "    data.append(df)\n",
        "    print('Asset Price Fetched:', df['asset'].iloc[0].upper())\n",
        "    time.sleep(3)\n",
        "\n",
        "cg_df = pd.concat(data, axis = 0)\n",
        "\n",
        "# Additional Calcs\n",
        "cg_df['vol/mcap'] = cg_df['volume'] / cg_df['mcap']\n",
        "\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "# Dune API Call - SAFE DEX Data\n",
        "# ==============================================================================\n",
        "# ==============================================================================\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction & handling\n",
        "# ==============================================================================\n",
        "\n",
        "query_id = 3777854  # SAFE - * - GMD - $SAFE DEX Trading Volume (Pool)\n",
        "\n",
        "df = spice.query(query_id, api_key=JTConfig.DUNE_API_KEY, refresh=True)\n",
        "query_result = df.to_pandas()\n",
        "\n",
        "# Make date index to resample and SUM data\n",
        "query_result['date'] = pd.to_datetime(query_result['block_date'])\n",
        "\n",
        "# Daily aggregation of numeric data\n",
        "dex_df = query_result.resample('D', kind = 'period', on = 'date').sum(numeric_only = True)\n",
        "\n",
        "# Select & Rename columns\n",
        "dex_df = dex_df[['trades', 'total_vol']]\n",
        "dex_df.columns = ['dex_trades', 'dex_vol']\n",
        "\n",
        "# ==============================================================================\n",
        "# Joining CG + DEX Data & final things\n",
        "# ==============================================================================\n",
        "\n",
        "# Add asset col for joining\n",
        "dex_df['asset'] = 'safe'\n",
        "\n",
        "# DF Merge - the total_df DataFrame will also be used later\n",
        "total_df = cg_df.merge(dex_df, how = 'left', on = ['date', 'asset'])\n",
        "\n",
        "# CEX Volume Calculations\n",
        "total_df['cex_vol'] = total_df['volume'] - total_df['dex_vol']\n",
        "\n",
        "# Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "total_df['etl_dt'] = JTConfig.ETL_NOW\n",
        "total_df['etl_dt'] = total_df['etl_dt'].dt.floor('s')\n",
        "\n",
        "# ==============================================================================\n",
        "# Data export\n",
        "# ==============================================================================\n",
        "\n",
        "market_data_df = total_df.copy()\n",
        "\n",
        "# Open sheet inside workbook\n",
        "sheet = wb.worksheet('market_data')\n",
        "\n",
        "# Reset index for all data to appear - symbols were the index\n",
        "market_data_df = market_data_df.reset_index()\n",
        "\n",
        "# Convert relevant columns in dataframe to str to export - plus additional NaN values to nan in string\n",
        "market_data_df = market_data_df.astype({'date': str, 'etl_dt': str})\n",
        "market_data_df = market_data_df.fillna('nan')\n",
        "\n",
        "# DataFrame export\n",
        "market_data_df_export_list = [market_data_df.columns.tolist()]  # Column labels\n",
        "market_data_df_export_list.extend(market_data_df.values.tolist())\n",
        "\n",
        "sheet.update(\"A1\", market_data_df_export_list)\n",
        "\n",
        "# Export to csv\n",
        "market_data_df.to_csv('market_data.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_72pOAd4BT8Q"
      },
      "source": [
        "## Dune - dex_fees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWXFFJs2BXu8"
      },
      "outputs": [],
      "source": [
        "# \"\"\"\n",
        "# Execute and get results of the Dune Query for DEX Fees\n",
        "# \"\"\"\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Request construction\n",
        "# # ==============================================================================\n",
        "\n",
        "# query_id = 3768733  # SAFE - * - GMD - $SAFE DEX Trading Fees\n",
        "\n",
        "# df = spice.query(query_id, refresh=True)\n",
        "# query_result = df.to_pandas()\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Request handling\n",
        "# # ==============================================================================\n",
        "\n",
        "# # Multiplty by current price to get current value of fees (for comparison with Revert)\n",
        "# query_result['fee_amount_usd_adj'] = query_result['token'].replace(regex = portfolio_tokens_v2['token_class'])  # replace token by the asset for price\n",
        "\n",
        "# \"\"\"Temporal Solution for Metis appearance\n",
        "# \"\"\"\n",
        "# query_result = query_result.loc[query_result['fee_amount_usd_adj'] not in ['Metis', 'OLAS']]\n",
        "\n",
        "# prices_ = prices_df.set_index('index')['price'].to_dict()  # get prices from the DataFrame\n",
        "\n",
        "# query_result['fee_amount_usd_adj'] = query_result['fee_amount_usd_adj'].replace(regex = prices_).astype(float)  # replace with prices\n",
        "# query_result['fee_amount_token'] = query_result['fee_amount_token'].replace('<nil>', 0)  # replace Dune nulls with 0\n",
        "# query_result['fee_amount_usd_adj'] = query_result['fee_amount_usd_adj'] * query_result['fee_amount_token'].astype(float)  # multiply amount by price\n",
        "\n",
        "# # Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "# query_result['etl_dt'] = etl_now\n",
        "# query_result['etl_dt'] = query_result['etl_dt'].dt.floor('s')\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Data export\n",
        "# # ==============================================================================\n",
        "\n",
        "# dex_fees = query_result.copy()\n",
        "\n",
        "# # Open sheet inside workbook\n",
        "# sheet = wb.worksheet('dex_fees')\n",
        "\n",
        "# # Convert relevant columns in dataframe to str to export - plus additional NaN values to nan in string\n",
        "# dex_fees = dex_fees.astype({'block_date': str, 'etl_dt': str})\n",
        "# dex_fees = dex_fees.fillna('nan')\n",
        "\n",
        "# # Reset index for all data to appear - symbols were the index\n",
        "# dex_fees = dex_fees.reset_index()\n",
        "\n",
        "# # DataFrame export\n",
        "# dex_fees_export_list = [dex_fees.columns.tolist()]  # Column labels\n",
        "# dex_fees_export_list.extend(dex_fees.values.tolist())\n",
        "# sheet.update(\"A1\", dex_fees_export_list)\n",
        "\n",
        "# # Export to csv\n",
        "# dex_fees.to_csv('dex_fees.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUpbs9252E7b"
      },
      "source": [
        "## Dune - dex_trades"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G68ouinTfRFm"
      },
      "source": [
        "- Requires executing the Market Data script before\n",
        "- Needed from query\n",
        "  - block_date\n",
        "  - blockchain\n",
        "  - label\n",
        "  - token_sold_symbol\n",
        "  - token_bought_amount\n",
        "  - token_sold_amount\n",
        "  - amount_usd\n",
        "  - tx_from"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "collapsed": true,
        "id": "3Xe_YCcl2EuI",
        "outputId": "0e1c8c58-25b1-4ceb-9f70-f3ec7f334e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-21-576369548.py:151: DeprecationWarning: The order of arguments in worksheet.update() has changed. Please pass values first and range_name secondor used named arguments (range_name=, values=)\n",
            "  sheet.update(\"A1\", dex_trades_exp_export_list)\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Seeks trading activity from Dune analytics - to gather data on it\n",
        "\"\"\"\n",
        "\n",
        "# ==============================================================================\n",
        "# Request construction\n",
        "# ==============================================================================\n",
        "\n",
        "query_id = 3768636  # SAFE - * - GMD - $SAFE DEX Trades (ref)\n",
        "\n",
        "df = spice.query(query_id, api_key=JTConfig.DUNE_API_KEY, refresh=True)\n",
        "query_result = df.to_pandas()  # it's received in a polars dataframe\n",
        "\n",
        "# get most recent query results using raw sql\n",
        "# df = spice.query('SELECT * FROM ethereum.blocks LIMIT 5')\n",
        "\n",
        "# ==============================================================================\n",
        "# Request handling\n",
        "# ==============================================================================\n",
        "\n",
        "# The dex_trades DataFrame will also be used in the other metrics analysis\n",
        "dex_trades = query_result.copy()\n",
        "\n",
        "# Convert 'block_date' to datetime - it has to be converted again later when setting index because of the dt.date applied first\n",
        "dex_trades['block_date'] = pd.to_datetime(dex_trades['block_date']).dt.date\n",
        "dex_trades = dex_trades.set_index(pd.to_datetime(dex_trades['block_date'])).drop('block_date', axis = 1)\n",
        "\n",
        "# Isolate only the SAFE numbers of the trades - with addresses of trades\n",
        "\n",
        "# SAFE Buys\n",
        "dt_buy = dex_trades.loc[dex_trades['token_bought_symbol'] == 'SAFE'].loc[:, ['blockchain', 'label', 'tx_from', 'token_bought_amount', 'amount_usd']]\n",
        "dt_buy['type'] = 'buy'\n",
        "dt_buy = dt_buy.loc[:, ['blockchain', 'label', 'type', 'tx_from', 'token_bought_amount', 'amount_usd']]\n",
        "dt_buy.columns = ['blockchain', 'label', 'type', 'address', 'amount', 'amount_usd']\n",
        "\n",
        "# SAFE Sells\n",
        "dt_sell = dex_trades.loc[dex_trades['token_sold_symbol'] == 'SAFE'].loc[:, ['blockchain', 'label', 'tx_from', 'token_sold_amount', 'amount_usd']]\n",
        "dt_sell['type'] = 'sell'\n",
        "dt_sell = dt_sell.loc[:, ['blockchain', 'label', 'type', 'tx_from', 'token_sold_amount', 'amount_usd']]\n",
        "dt_sell.columns = ['blockchain', 'label', 'type', 'address', 'amount', 'amount_usd']\n",
        "\n",
        "# Join DFs for complete one - will also be used in the other metrics analysis\n",
        "dt = pd.concat([dt_buy, dt_sell], axis = 0)\n",
        "\n",
        "# ==============================================================================\n",
        "# Ethereum statistics of trades to export to the JT sheet\n",
        "# ==============================================================================\n",
        "\n",
        "# Filter by blockchain and trade size wanting to be filtered (defined below in the trading activity analysis)\n",
        "dt_eth = dt.loc[(dt['blockchain'] == 'ethereum') & (dt['amount'] < 100000)].copy()  # Rationale for 100k SAFE size is explained in Cleaning section below\n",
        "\n",
        "# Get metrics daily\n",
        "aggregations = [\n",
        "    ('trades', 'count'),\n",
        "    ('min', 'min'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('max', 'max'),\n",
        "    ('0.05', lambda x: x.quantile(0.05)),\n",
        "    ('0.90', lambda x: x.quantile(0.90)),\n",
        "    ('0.95', lambda x: x.quantile(0.95)),\n",
        "    ('0.98', lambda x: x.quantile(0.98)),\n",
        "    ('0.99', lambda x: x.quantile(0.99))\n",
        "]\n",
        "\n",
        "dt_eth_daily = dt_eth.resample('D')['amount'].agg(aggregations)  # Remember this has removed the 100k trades\n",
        "\n",
        "# Make sure NaN are treated as 0\n",
        "dt_eth_daily = dt_eth_daily.fillna(0)\n",
        "\n",
        "# Rollsing statistics - the percentiles targetted here depend on the analysis below\n",
        "dt_eth_daily['7d_mean'] = dt_eth_daily['mean'].rolling(window = 7).mean()\n",
        "dt_eth_daily['14d_mean'] = dt_eth_daily['mean'].rolling(window = 14).mean()\n",
        "dt_eth_daily['30d_mean'] = dt_eth_daily['mean'].rolling(window = 30).mean()\n",
        "\n",
        "dt_eth_daily['0.95_30d'] = dt_eth_daily['0.95'].rolling(window = 30).mean()\n",
        "dt_eth_daily['0.98_30d'] = dt_eth_daily['0.98'].rolling(window = 30).mean()\n",
        "dt_eth_daily['0.99_30d'] = dt_eth_daily['0.99'].rolling(window = 30).mean()\n",
        "\n",
        "# Add blockchain\n",
        "dt_eth_daily['blockchain'] = 'ethereum'\n",
        "\n",
        "# Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "dt_eth_daily['etl_dt'] = JTConfig.ETL_NOW\n",
        "dt_eth_daily['etl_dt'] = dt_eth_daily['etl_dt'].dt.floor('s')\n",
        "\n",
        "# ==============================================================================\n",
        "# Gnosis statistics of trades to export to the JT sheet\n",
        "# ==============================================================================\n",
        "\n",
        "# Filter by blockchain and trade size wanting to be filtered (defined below in the trading activity analysis)\n",
        "dt_gno = dt.loc[dt['blockchain'] == 'gnosis'].copy()  # Rationale for 100k SAFE size is explained in Cleaning section below\n",
        "\n",
        "# Descriptive Statistics\n",
        "dt_gno.describe()\n",
        "\n",
        "# Get metrics daily\n",
        "aggregations = [\n",
        "    ('trades', 'count'),\n",
        "    ('min', 'min'),\n",
        "    ('median', 'median'),\n",
        "    ('mean', 'mean'),\n",
        "    ('max', 'max'),\n",
        "    ('0.05', lambda x: x.quantile(0.05)),\n",
        "    ('0.90', lambda x: x.quantile(0.90)),\n",
        "    ('0.95', lambda x: x.quantile(0.95)),\n",
        "    ('0.98', lambda x: x.quantile(0.98)),\n",
        "    ('0.99', lambda x: x.quantile(0.99))\n",
        "]\n",
        "\n",
        "dt_gno_daily = dt_gno.resample('D')['amount'].agg(aggregations)  # Remember this has removed the 100k trades\n",
        "\n",
        "# Make sure NaN are treated as 0\n",
        "dt_gno_daily = dt_gno_daily.fillna(0)\n",
        "\n",
        "# Rollsing statistics - the percentiles targetted here depend on the analysis below\n",
        "dt_gno_daily['7d_mean'] = dt_gno_daily['mean'].rolling(window = 7).mean()\n",
        "dt_gno_daily['14d_mean'] = dt_gno_daily['mean'].rolling(window = 14).mean()\n",
        "dt_gno_daily['30d_mean'] = dt_gno_daily['mean'].rolling(window = 30).mean()\n",
        "\n",
        "dt_gno_daily['0.95_30d'] = dt_gno_daily['0.95'].rolling(window = 30).mean()\n",
        "dt_gno_daily['0.98_30d'] = dt_gno_daily['0.98'].rolling(window = 30).mean()\n",
        "dt_gno_daily['0.99_30d'] = dt_gno_daily['0.99'].rolling(window = 30).mean()\n",
        "\n",
        "# Add blockchain\n",
        "dt_gno_daily['blockchain'] = 'gnosis'\n",
        "\n",
        "# Add etl_dt (etl datetime for reference of when info was fetched)\n",
        "dt_gno_daily['etl_dt'] = JTConfig.ETL_NOW\n",
        "dt_gno_daily['etl_dt'] = dt_gno_daily['etl_dt'].dt.floor('s')\n",
        "\n",
        "# ==============================================================================\n",
        "# Combined Data export\n",
        "# ==============================================================================\n",
        "\n",
        "dex_trades_exp = pd.concat([dt_eth_daily, dt_gno_daily], axis = 0)\n",
        "\n",
        "# Open sheet inside workbook\n",
        "sheet = wb.worksheet('dex_trades')\n",
        "\n",
        "# Reset index for all data to appear - date is index\n",
        "dex_trades_exp = dex_trades_exp.reset_index()\n",
        "\n",
        "# Convert relevant columns in dataframe to str to export - plus additional NaN values to nan in string\n",
        "dex_trades_exp = dex_trades_exp.astype({'block_date': str, 'etl_dt': str})\n",
        "dex_trades_exp = dex_trades_exp.fillna('nan')\n",
        "\n",
        "# DataFrame export\n",
        "dex_trades_exp_export_list = [dex_trades_exp.columns.tolist()]  # Column labels\n",
        "dex_trades_exp_export_list.extend(dex_trades_exp.values.tolist())\n",
        "sheet.update(\"A1\", dex_trades_exp_export_list)\n",
        "\n",
        "# Export to csv\n",
        "dex_trades_exp.to_csv('dex_trades.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aBllUiMYmU0"
      },
      "outputs": [],
      "source": [
        "dex_trades_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkC9ydZcsHOI"
      },
      "outputs": [],
      "source": [
        "dt_eth_daily"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXsEDduuEtsX"
      },
      "source": [
        "# Other Metrics Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV7uPCXaDu3f"
      },
      "source": [
        "## SAFE/ETH Max Total Weekly Drawdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-_tdL5ijt7K"
      },
      "outputs": [],
      "source": [
        "# Uses total_df from market_data section\n",
        "\n",
        "safe_eth = total_df.loc[total_df['asset'] == 'safe']['close'] / total_df.loc[total_df['asset'] == 'eth']['close']\n",
        "\n",
        "safe_eth_w = safe_eth / safe_eth.shift(7) - 1  # Moves one row downards\n",
        "safe_eth_w.describe()  # Looking for max eth_safe depreciation weekly to put the ranges in the Uni V3 pool - we rebalance weekly so we have to aim to cover max possible deppreciation with desired liquidity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pE8B0X6Nb3Nw"
      },
      "outputs": [],
      "source": [
        "safe_eth_w_df = safe_eth_w.to_frame()\n",
        "\n",
        "safe_eth_w_df.loc[safe_eth_w_df['close'] == safe_eth_w_df.describe().loc['min','close']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMqYnoapsiYG"
      },
      "source": [
        "## Mainnet DEX Trading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD-0479ooA12"
      },
      "outputs": [],
      "source": [
        "# Uses dex_trades dataframe from dex_trades section\n",
        "\n",
        "# EXTENDED Analysis of ETHEREUM trades\n",
        "\n",
        "dt_eth_ext = dt.loc[dt['blockchain'] == 'ethereum']\n",
        "\n",
        "dt_eth_ext.describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcTTWsC6x8PM"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of trades to see if there are outliers (I want to target liquidity based on normal data)\n",
        "\n",
        "# Creating 2x2 subplots with histograms for each column\n",
        "# fig, axs = plt.subplots(1, 2, figsize=(14, 8))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_axes(rect = [1, 1, 1, 1])\n",
        "\n",
        "ax.hist(dt_eth_ext['amount'], bins=150, edgecolor='black')\n",
        "ax.set_title('Trade Size Distribution')\n",
        "ax.set_xlabel('Trade Size')\n",
        "ax.set_ylabel('Number Of Trades')\n",
        "\n",
        "# Set tick sizes to see better\n",
        "ticks = [x for x in range(0, int(dt_eth_ext['amount'].max()), 10000)]\n",
        "ax.set_xticks(ticks, labels=None)  # Se pasan los ticks en una lista/array\n",
        "ax.tick_params('x', labelrotation = 45)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cM5RP431Y5w"
      },
      "outputs": [],
      "source": [
        "# Since it's a time series we can also check the average trade size for particular dates\n",
        "\n",
        "dt_eth_ext_daily = dt_eth_ext.resample('D')['amount'].agg(['mean', 'median', 'max'])\n",
        "\n",
        "dt_eth_ext_daily[['mean', 'median', 'max']].plot(figsize=(12,6))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDQNh6Ky5DP8"
      },
      "source": [
        "#### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tur9zbIZvo2-"
      },
      "outputs": [],
      "source": [
        "# Clean the dt_ext\n",
        "\n",
        "# Take out the outliers seen above\n",
        "\n",
        "# There seems to be only four dates with a maximum above 100,000 SAFE\n",
        "pprint.pp(dt_eth_ext_daily.loc[dt_eth_ext_daily['max'] > 100000].index.unique().values)\n",
        "print('')\n",
        "dt_eth_ext_daily.loc[dt_eth_ext_daily['max'] < 100000].plot(figsize=(12,6))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2RU2cdM5CHW"
      },
      "outputs": [],
      "source": [
        "# What happens if we only take out the trades above that size\n",
        "\n",
        "# As a reminder - We want to target 30d mean and the surplus from there to either the 0.95, 0.98, 0.99 rolling 30d mean as well\n",
        "\n",
        "dt_ = dt.loc[dt['blockchain'] == 'ethereum'].copy()\n",
        "\n",
        "\n",
        "print('Amount of trades above 100k SAFE:', dt_.loc[dt_['amount'] > 100000].size)\n",
        "print('Out of a total of:', dt_.size)\n",
        "print('Trades >100k SAFE over total Trades:', f\"{dt_.loc[dt_['amount'] > 100000].size / dt.size * 100}%\")\n",
        "\n",
        "print('')\n",
        "print('Statistics without the outliers')\n",
        "dt_.loc[dt_['amount'] < 100000].describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2vU89GQs1K-"
      },
      "outputs": [],
      "source": [
        "# Addresses per trading bracket\n",
        "\n",
        "# Define the percentiles we are interested in\n",
        "percentiles = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "# Calculate the percentile values for amount\n",
        "percentile_values = dt_eth_ext['amount'].quantile(percentiles)\n",
        "\n",
        "# Dictionary to hold the unique addresses for each percentile range\n",
        "unique_addresses_by_percentile = {}\n",
        "\n",
        "# For each percentile, filter the dataframe and find unique 'label'\n",
        "for p in percentiles:\n",
        "    threshold = percentile_values[p]\n",
        "    filtered_df = dt_eth_ext.loc[dt_eth_ext['amount'] >= threshold]\n",
        "    unique_addresses = filtered_df['address'].nunique()\n",
        "    unique_addresses_by_percentile[f'{int(p*100)}th Percentile'] = unique_addresses\n",
        "\n",
        "# Convert the results into a DataFrame for better readability\n",
        "unique_addresses_df = pd.DataFrame(dict([(key, pd.Series(value)) for key, value in unique_addresses_by_percentile.items()])).T\n",
        "\n",
        "unique_addresses_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOy96bG-ovr3"
      },
      "source": [
        "### More Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjn6KsPDo0st"
      },
      "outputs": [],
      "source": [
        "dt_eth_ext.resample('D').mean(numeric_only = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYZ5PSY0p1XA"
      },
      "outputs": [],
      "source": [
        "\"\"\"Total size and value traded\n",
        "\"\"\"\n",
        "\n",
        "dt_eth_ext.resample('D').sum(numeric_only = True).loc['2024-07':].plot(figsize=(12,6))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoY9_uhsR1Fs"
      },
      "source": [
        "## Gnosis Chain DEX Trading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E98VWhxaUrHt"
      },
      "outputs": [],
      "source": [
        "# Uses dex_trades dataframe from dex_trades section\n",
        "\n",
        "# EXTENDED Analysis of ETHEREUM trades\n",
        "\n",
        "dt_gno_ext = dt.loc[dt['blockchain'] == 'gnosis']\n",
        "\n",
        "dt_gno_ext.describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRQdhVLlUznf"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of trades to see if there are outliers (I want to target liquidity based on normal data)\n",
        "\n",
        "# Creating 2x2 subplots with histograms for each column\n",
        "# fig, axs = plt.subplots(1, 2, figsize=(14, 8))\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "ax = fig.add_axes(rect = [1, 1, 1, 1])\n",
        "\n",
        "ax.hist(dt_gno_ext['amount'], bins=150, edgecolor='black')\n",
        "ax.set_title('Trade Size Distribution')\n",
        "ax.set_xlabel('Trade Size')\n",
        "ax.set_ylabel('Number Of Trades')\n",
        "\n",
        "# Set tick sizes to see better\n",
        "ticks = [x for x in range(0, int(dt_gno_ext['amount'].max()), 500)]\n",
        "ax.set_xticks(ticks, labels=None)  # Se pasan los ticks en una lista/array\n",
        "ax.tick_params('x', labelrotation = 45)\n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWL1iFEuVq8p"
      },
      "outputs": [],
      "source": [
        "# Since it's a time series we can also check the average trade size for particular dates\n",
        "\n",
        "dt_gno_ext_daily = dt_gno_ext.resample('D')['amount'].agg(['mean', 'median', 'max'])\n",
        "\n",
        "dt_gno_ext_daily[['mean', 'median', 'max']].plot(figsize=(12,6))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KQxGxlTWEoD"
      },
      "source": [
        "### Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZeVW-u-lVxgI"
      },
      "outputs": [],
      "source": [
        "# Clean the dt_ext\n",
        "\n",
        "# Take out the outliers seen above\n",
        "\n",
        "# There seems to be only four dates with a maximum above 100,000 SAFE\n",
        "pprint.pp(dt_gno_ext_daily.loc[dt_gno_ext_daily['max'] > 8000].index.unique().values)\n",
        "print('')\n",
        "dt_gno_ext_daily.loc[dt_gno_ext_daily['max'] < 8000].plot(figsize=(12,6))\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhnVYR8uWHiA"
      },
      "outputs": [],
      "source": [
        "# What happens if we only take out the trades above that size\n",
        "\n",
        "# As a reminder - We want to target 30d mean and the surplus from there to either the 0.95, 0.98, 0.99 rolling 30d mean as well\n",
        "\n",
        "dt_ = dt.loc[dt['blockchain'] == 'gnosis'].copy()\n",
        "\n",
        "\n",
        "print('Amount of trades above 8k SAFE:', dt_.loc[dt_['amount'] > 8000].size)\n",
        "print('Out of a total of:', dt_.size)\n",
        "print('Trades >8k SAFE over total Trades:', f\"{dt_.loc[dt_['amount'] > 8000].size / dt.size * 100}%\")\n",
        "\n",
        "print('')\n",
        "print('Statistics without the outliers')\n",
        "dt_.loc[dt_['amount'] < 8000].describe(percentiles=[0.25, 0.50, 0.75, 0.9, 0.95, 0.98, 0.99])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5U9FyC6Wblr"
      },
      "source": [
        "## Extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdiNn2wNWcvo"
      },
      "outputs": [],
      "source": [
        "dt.loc[dt['amount'] > 10000].loc['2024-09-17':]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAu2XmxIFz6-"
      },
      "source": [
        "# Close Session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJF3Bu5zF37w"
      },
      "outputs": [],
      "source": [
        "# wb.client.session.close()  # Close the session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbqLWAp_-OLN"
      },
      "source": [
        "# Security Backup (WIP)\n",
        "\n",
        "Create a csv file backup in Drive for each table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMb9gawLDmgT"
      },
      "source": [
        "# Archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dn1ydTf7Dn6p"
      },
      "outputs": [],
      "source": [
        "# # ==============================================================================\n",
        "# # SAFE DEX Data from Dune Analytics\n",
        "# # ==============================================================================\n",
        "\n",
        "# query_id = 3777854  # SAFE - * - GMD - $SAFE DEX Trading Volume (Pool)\n",
        "\n",
        "# headers = {'X-DUNE-API-KEY': config['api_keys']['dune']['key']}\n",
        "\n",
        "# delay = 10  # time delay in seconds for checking succesfull execution\n",
        "# q = 0\n",
        "\n",
        "# # Request for executing the query\n",
        "\n",
        "# base_url = f\"https://api.dune.com/api/v1/query/{query_id}/execute\"\n",
        "\n",
        "# try:\n",
        "#     r = requests.post(base_url, headers=headers)\n",
        "#     r.raise_for_status()  # Raise an exception for unsuccessful HTTP status codes\n",
        "\n",
        "#     execution_id = r.json()['execution_id']  # Get exec id for status check\n",
        "#     time.sleep(1)\n",
        "\n",
        "#     print(\"Request successful!\")\n",
        "# except requests.exceptions.RequestException as e:\n",
        "#     print(\"An error occurred:\", e)\n",
        "\n",
        "# # Checking the query execution status\n",
        "\n",
        "# base_url = f\"https://api.dune.com/api/v1/execution/{execution_id}/status\"\n",
        "\n",
        "# while True:\n",
        "#   r = requests.get(base_url, headers=headers)\n",
        "\n",
        "#   if r.json()['is_execution_finished']:\n",
        "#     print(\"Execution Finished\")\n",
        "#     break\n",
        "\n",
        "#   print(\"Awating Execution:\", str(q), 's')\n",
        "#   time.sleep(delay)\n",
        "#   q += delay\n",
        "\n",
        "# # Getting the query results - with Dune Client\n",
        "\n",
        "# dune = DuneClient(\n",
        "#     api_key = config['api_keys']['dune']['key'],\n",
        "#     base_url = \"https://api.dune.com\",\n",
        "#     request_timeout=(300) # request will time out after 300 seconds\n",
        "# )\n",
        "\n",
        "# query_result = dune.get_latest_result_dataframe(\n",
        "#     query = query_id\n",
        "#     # # filter for users account more than a month old and more than bottom active tier\n",
        "#     # , filters=\"account_age > 30 and fid_active_tier > 1\"\n",
        "#     # # sort result by number of channels they are follow in descending order\n",
        "#     # , sort_by=[\"channels desc\"]\n",
        "# )\n",
        "\n",
        "# # Make date index to resample and SUM data\n",
        "# query_result['date'] = pd.to_datetime(query_result['block_date'])\n",
        "\n",
        "# # Daily aggregation of numeric data\n",
        "# dex_df = query_result.resample('D', kind = 'period', on = 'date').sum(numeric_only = True)\n",
        "\n",
        "# # Select & Rename columns\n",
        "# dex_df = dex_df[['trades', 'total_vol']]\n",
        "# dex_df.columns = ['dex_trades', 'dex_vol']\n",
        "\n",
        "# # ==============================================================================\n",
        "# # Joining CG + DEX Data & final things\n",
        "# # ==============================================================================\n",
        "\n",
        "# # Add asset col for joining\n",
        "# dex_df['asset'] = 'safe'\n",
        "\n",
        "# dex_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMIrVaW4ldpJ"
      },
      "source": [
        "## CoW AMM\n",
        "\n",
        "- https://docs.cow.fi/cow-protocol/reference/apis/orderbook\n",
        "- https://explorer.cow.fi/address/0x027e1cbf2c299cba5eb8a2584910d04f1a8aa403\n",
        "\n",
        "- https://colab.research.google.com/drive/1Si3tBKl004UVxGz9I9spEMUJpc7IMkZs?usp=sharing#scrollTo=8f3iYPODSXbP\n",
        "- https://colab.research.google.com/drive/1UKUNSMFoOLeg9fy1v-TYnymHSn7N1ghq?usp=sharing#scrollTo=8f3iYPODSXbP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KrRlRNXmb2w"
      },
      "outputs": [],
      "source": [
        "# import requests\n",
        "# import pandas as pd\n",
        "\n",
        "# \"\"\"\n",
        "# Get CoW AMM orders (executed and failed)\n",
        "# \"\"\"\n",
        "\n",
        "# account_address = '0x027e1cbf2c299cba5eb8a2584910d04f1a8aa403'\n",
        "\n",
        "# base_url = f\"https://api.cow.fi/mainnet/api/v1/account/{account_address}/orders\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}